{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using streaming mode\n",
    "dataset = datasets.load_dataset(\"Lichess/standard-chess-games\", split=\"train\", streaming=True)\n",
    "\n",
    "# Get a small sample of 10 games\n",
    "sample = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if i <= 10000:\n",
    "        continue\n",
    "    if i > 200000:\n",
    "        break\n",
    "    sample.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a slice from item 10001 to item 200000...\n",
      "Collected 10000 items from the slice...\n",
      "Collected 20000 items from the slice...\n",
      "Collected 30000 items from the slice...\n",
      "Collected 40000 items from the slice...\n",
      "Collected 50000 items from the slice...\n",
      "Collected 60000 items from the slice...\n",
      "Collected 70000 items from the slice...\n",
      "Collected 80000 items from the slice...\n",
      "Collected 90000 items from the slice...\n",
      "Collected 100000 items from the slice...\n",
      "Collected 110000 items from the slice...\n",
      "Collected 120000 items from the slice...\n",
      "Collected 130000 items from the slice...\n",
      "Collected 140000 items from the slice...\n",
      "Collected 150000 items from the slice...\n",
      "Collected 160000 items from the slice...\n",
      "Collected 170000 items from the slice...\n",
      "Collected 180000 items from the slice...\n",
      "Collected 190000 items from the slice...\n",
      "Finished. Total items in sample: 190000\n",
      "First item of the sample (which was the 10001st item of the dataset): {'Event': 'Rated Bullet game', 'Site': 'https://lichess.org/tcobg87g', 'White': 'ABUELOENTANGA', 'Black': 'jhipolito', 'Result': '0-1', 'WhiteTitle': None, 'BlackTitle': None, 'WhiteElo': 1838, 'BlackElo': 1980, 'WhiteRatingDiff': -7, 'BlackRatingDiff': 7, 'UTCDate': datetime.date(2013, 1, 3), 'UTCTime': datetime.time(17, 54, 35), 'ECO': 'B02', 'Opening': 'Alekhine Defense: Two Pawn Attack', 'Termination': 'Normal', 'TimeControl': '60+0', 'movetext': '1. e4 Nf6 2. e5 Nd5 3. c4 Nb6 4. Nc3 d6 5. exd6 exd6 6. Nf3 Nc6 7. Be2 Bg4 8. O-O Bxf3 9. Bxf3 Nxc4 10. b3 Nc4e5 11. Bb2 Be7 12. Re1 O-O 13. d4 Nxf3+ 14. Qxf3 Bf6 15. Ne4 Bxd4 16. Bxd4 Nxd4 17. Qg3 f5 18. Ng5 Re8 19. Qh3 h6 20. Rxe8+ Qxe8 21. Nf3 Nxf3+ 22. Qxf3 Qe4 23. Qc3 c6 24. Re1 Qg4 25. h3 Qg5 26. Re6 d5 27. b4 Rf8 28. b5 cxb5 29. Qc7 f4 30. Qd6 f3 31. Rg6 Qc1+ 32. Kh2 fxg2 33. Rxg2 Qf4+ 34. Rg3 Qxd6 0-1'}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import itertools\n",
    "\n",
    "# Load using streaming mode\n",
    "dataset_iterable = datasets.load_dataset(\"Lichess/standard-chess-games\", split=\"train\", streaming=True)\n",
    "\n",
    "# Define the slice (0-indexed)\n",
    "# You want to skip the first 10001 items (indices 0 to 10000)\n",
    "# And take items up to index 200000 (exclusive of 200001 if using as stop)\n",
    "# So, start index for islice = 10001\n",
    "# Stop index for islice = 200001 (to include item at index 200000)\n",
    "# This means you're interested in dataset items that would correspond to `i` from 10001 to 200000 in your original loop.\n",
    "\n",
    "# Original loop:\n",
    "# i = 0 to 10000 -> continue (skip 10001 items: 0 to 10000)\n",
    "# i = 10001 to 200000 -> append (190000 items)\n",
    "# i = 200001 -> break\n",
    "\n",
    "start_index_for_slice = 10001 # Corresponds to `i` starting at 10001\n",
    "stop_index_for_slice = 200001 # Corresponds to `i` up to 200000, so `islice` stops *before* 200001\n",
    "\n",
    "print(f\"Creating a slice from item {start_index_for_slice} to item {stop_index_for_slice -1}...\")\n",
    "\n",
    "# itertools.islice(iterable, start, stop, step)\n",
    "# We are taking items from the (start_index_for_slice)-th position up to (stop_index_for_slice-1)-th position.\n",
    "# If dataset_iterable were a list, it would be dataset_iterable[start_index_for_slice : stop_index_for_slice]\n",
    "sliced_data = itertools.islice(dataset_iterable, start_index_for_slice, stop_index_for_slice)\n",
    "\n",
    "# Get a sample (now from the sliced_data)\n",
    "sample = []\n",
    "for i, row in enumerate(sliced_data): # enumerate here will be 0-indexed for the slice\n",
    "    sample.append(row)\n",
    "    if (i + 1) % 10000 == 0: # Print progress every 10000 items collected from the slice\n",
    "        print(f\"Collected {i+1} items from the slice...\")\n",
    "\n",
    "print(f\"Finished. Total items in sample: {len(sample)}\")\n",
    "if sample:\n",
    "    print(\"First item of the sample (which was the 10001st item of the dataset):\", sample[0])\n",
    "    # print(\"Last item of the sample (which was the 200000th item of the dataset):\", sample[-1]) # only if len(sample) == expected_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import chess\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------- CONFIG: integer IDs for piece types ------------------- #\n",
    "piece_label_to_id = {\n",
    "    \"WR\": 1, \"WN\": 2, \"WB1\": 3, \"WQ\": 4, \"WK\": 5, \"WB2\": 6, \"WP\": 7,\n",
    "    \"BP\": 8, \"BR\": 9, \"BN\": 10, \"BB1\": 11, \"BQ\": 12, \"BK\": 13, \"BB2\": 14,\n",
    "}\n",
    "\n",
    "# ------------------- INITIAL BOARD ------------------- #\n",
    "initial_map = {\n",
    "    0: \"WR\", 1: \"WN\", 2: \"WB1\", 3: \"WQ\", 4: \"WK\", 5: \"WB2\", 6: \"WN\", 7: \"WR\",\n",
    "    8: \"WP\", 9: \"WP\", 10: \"WP\", 11: \"WP\", 12: \"WP\", 13: \"WP\", 14: \"WP\", 15: \"WP\",\n",
    "    48: \"BP\", 49: \"BP\", 50: \"BP\", 51: \"BP\", 52: \"BP\", 53: \"BP\", 54: \"BP\", 55: \"BP\",\n",
    "    56: \"BR\", 57: \"BN\", 58: \"BB1\", 59: \"BQ\", 60: \"BK\", 61: \"BB2\", 62: \"BN\", 63: \"BR\",\n",
    "}\n",
    "\n",
    "# ------------------- Move Parser ------------------- #\n",
    "_move_num = re.compile(r\"^\\d+\\.(\\.\\.)?$\")\n",
    "\n",
    "def san_stream(movetext: str):\n",
    "    for tok in movetext.replace(\"\\n\", \" \").split():\n",
    "        if _move_num.match(tok) or tok in {\"1-0\", \"0-1\", \"1/2-1/2\", \"*\"}:\n",
    "            continue\n",
    "        yield tok\n",
    "\n",
    "# ------------------- Convert mapping to tensor ------------------- #\n",
    "def mapping_to_tensor(mapping: dict) -> torch.LongTensor:\n",
    "    return torch.tensor([\n",
    "        piece_label_to_id.get(mapping.get(i), 0) for i in range(64)\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "# ------------------- Determine winner string ------------------- #\n",
    "def result_to_winner(result: str) -> str:\n",
    "    if result == \"1-0\":\n",
    "        return \"white\"\n",
    "    elif result == \"0-1\":\n",
    "        return \"black\"\n",
    "    else:\n",
    "        return \"draw\"\n",
    "    \n",
    "    \n",
    "def create_dataset_from_games(game_dicts: list[dict]) -> list[dict]:\n",
    "    full_dataset = []\n",
    "\n",
    "    for game in game_dicts:\n",
    "        try:\n",
    "            board = chess.Board()\n",
    "            mapping = initial_map.copy()\n",
    "            states = [mapping_to_tensor(mapping)]\n",
    "            turns = [\"white\"]  # starting with white\n",
    "            move_vectors = []\n",
    "\n",
    "            for san in san_stream(game[\"movetext\"]):\n",
    "                move = board.parse_san(san)\n",
    "                from_sq, to_sq = move.from_square, move.to_square\n",
    "                move_vector = [from_sq, to_sq]\n",
    "\n",
    "                # Update mapping\n",
    "                moving_id = mapping.pop(from_sq, None)\n",
    "\n",
    "                if to_sq in mapping:\n",
    "                    mapping.pop(to_sq)\n",
    "\n",
    "                if board.is_en_passant(move):\n",
    "                    ep_target = to_sq + (-8 if board.turn else 8)\n",
    "                    mapping.pop(ep_target, None)\n",
    "\n",
    "                mapping[to_sq] = moving_id\n",
    "\n",
    "                # Skip rook move, just rely on king's move in castling\n",
    "                board.push(move)\n",
    "\n",
    "                states.append(mapping_to_tensor(mapping))\n",
    "                turns.append(\"white\" if board.turn else \"black\")\n",
    "                move_vectors.append(move_vector)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        winner = result_to_winner(game[\"Result\"])\n",
    "\n",
    "        for i in range(len(states) - 1):\n",
    "            full_dataset.append({\n",
    "                \"input\": states[i],\n",
    "                \"output\": torch.tensor(move_vectors[i], dtype=torch.long),  # output is [from_sq, to_sq]\n",
    "                \"turn\": turns[i],\n",
    "                \"winner\": winner\n",
    "            })\n",
    "\n",
    "    return full_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_dataset_from_games(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12761032"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 1/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 134899.37 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 87.82ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 2/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:04<00:00, 122882.49 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 96.46ba/s] \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 3/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 128203.40 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 91.03ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 4/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:04<00:00, 124649.30 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 89.68ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 5/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 130130.45 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 93.07ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:08<00:00,  8.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 6/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 132578.17 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:06<00:00, 79.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:08<00:00,  8.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 7/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 144375.11 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 90.81ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 8/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 135977.41 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 93.39ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 9/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 127751.99 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 85.68ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 10/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 136505.42 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:04<00:00, 100.90ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:05<00:00,  5.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 11/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 128717.78 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 92.35ba/s] \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 12/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 140525.22 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 88.67ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 13/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 142392.48 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 92.16ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 14/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 130779.47 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 89.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 15/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 139212.59 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 92.84ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 16/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 140951.54 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 92.00ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 17/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 142019.22 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:07<00:00, 65.99ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:08<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 18/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 137322.37 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 91.22ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 19/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 145571.34 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 93.37ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 20/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 130646.80 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:10<00:00, 48.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:12<00:00, 12.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 21/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 147834.50 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 92.47ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 22/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 140196.23 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 92.92ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 23/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 148235.16 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 93.23ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 24/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 146291.88 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 91.94ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 25/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500000/500000 [00:03<00:00, 147139.62 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:05<00:00, 93.20ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 26/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 261032/261032 [00:01<00:00, 143204.73 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 262/262 [00:02<00:00, 90.25ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "# ------------------- CONFIG ------------------- #\n",
    "HUB_PATH = \"youngchiller40/chessset2\"\n",
    "SAVE_DIR = \"hf_chess_tmp\"\n",
    "SHARD_SIZE = 500_000\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------- Shard & Push ------------------- #\n",
    "num_shards = (len(ds) + SHARD_SIZE - 1) // SHARD_SIZE\n",
    "\n",
    "for shard_idx in range(num_shards):\n",
    "    print(f\"Processing shard {shard_idx + 1}/{num_shards}...\")\n",
    "\n",
    "    start = shard_idx * SHARD_SIZE\n",
    "    end = min((shard_idx + 1) * SHARD_SIZE, len(ds))\n",
    "    shard_data = ds[start:end]\n",
    "\n",
    "    # Convert tensors to lists for HF compatibility\n",
    "    hf_ready = [{\n",
    "        \"input\": d[\"input\"].tolist(),\n",
    "        \"output\": d[\"output\"].tolist(),\n",
    "        \"turn\": d[\"turn\"],\n",
    "        \"winner\": d[\"winner\"]\n",
    "    } for d in shard_data]\n",
    "\n",
    "    new_ds = Dataset.from_list(hf_ready)\n",
    "\n",
    "    # Save locally and then push\n",
    "    shard_path = os.path.join(SAVE_DIR, f\"shard_{shard_idx:04d}\")\n",
    "    new_ds.save_to_disk(shard_path)\n",
    "\n",
    "    # Reload and push to Hugging Face Hub\n",
    "    ds_loaded = load_from_disk(shard_path)\n",
    "    shard_split_name = f\"train_shard{shard_idx:04d}\"  # <-- THIS FIXES THE ERROR\n",
    "    ds_loaded.push_to_hub(HUB_PATH, split=shard_split_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
