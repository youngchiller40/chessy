{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "games = [{'Event': 'Rated Classical game',\n",
    " 'Site': 'https://lichess.org/j1dkb5dw',\n",
    " 'White': 'BFG9k',\n",
    " 'Black': 'mamalak',\n",
    " 'Result': '1-0',\n",
    " 'WhiteTitle': None,\n",
    " 'BlackTitle': None,\n",
    " 'WhiteElo': 1639,\n",
    " 'BlackElo': 1403,\n",
    " 'WhiteRatingDiff': 5,\n",
    " 'BlackRatingDiff': -8,\n",
    " 'UTCDate': datetime.date(2012, 12, 31),\n",
    " 'UTCTime': datetime.time(23, 1, 3),\n",
    " 'ECO': 'C00',\n",
    " 'Opening': 'French Defense: Normal Variation',\n",
    " 'Termination': 'Normal',\n",
    " 'TimeControl': '600+8',\n",
    " 'movetext': '1. e4 e6 2. d4 b6 3. a3 Bb7 4. Nc3 Nh6 5. Bxh6 gxh6 6. Be2 Qg5 7. Bg4 h5 8. Nf3 Qg6 9. Nh4 Qg5 10. Bxh5 Qxh4 11. Qf3 Kd8 12. Qxf7 Nc6 13. Qe8# 1-0'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "\n",
    "huggingface_hub.login(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 23/23 [00:16<00:00,  1.39files/s]\n",
      "Generating train split: 100%|██████████| 40885661/40885661 [01:11<00:00, 572760.22 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds = datasets.load_dataset('youngchiller40/chess-combined-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test (e.g., 90/10)\n",
    "split_dataset = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# legacy training block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 1.  Hyper‑params & device\n",
    "# ──────────────────────────────────────────────\n",
    "EMB_D    = 1024\n",
    "BATCH_SZ = 512\n",
    "LR       = 2e-4\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2.  Rotary Positional Embedding\n",
    "# ──────────────────────────────────────────────\n",
    "def build_rope_tables(seq_len: int, dim: int, device):\n",
    "    half = dim // 2\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(half, device=device) / half))\n",
    "    ang = torch.arange(seq_len, device=device).float().unsqueeze(1) * inv_freq[None, :]\n",
    "    return ang.sin(), ang.cos()\n",
    "\n",
    "rope_sin, rope_cos = build_rope_tables(64, EMB_D, device)\n",
    "rope_sin.requires_grad_(False)\n",
    "rope_cos.requires_grad_(False)\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    sin = sin.unsqueeze(0)\n",
    "    cos = cos.unsqueeze(0)\n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd  = x[..., 1::2]\n",
    "    rot_even = x_even * cos - x_odd * sin\n",
    "    rot_odd  = x_even * sin + x_odd * cos\n",
    "    x[..., 0::2] = rot_even\n",
    "    x[..., 1::2] = rot_odd\n",
    "    return x\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3.  Learnable Weights\n",
    "# ──────────────────────────────────────────────\n",
    "piece_tensors = torch.nn.ParameterList([\n",
    "    torch.nn.Parameter(torch.randn(EMB_D, device=device)) for _ in range(15)\n",
    "])\n",
    "turn_weights = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "queryW       = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "keyW         = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "valueW       = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "\n",
    "# MLP Head: outputs 64 logits per square\n",
    "mlp_head = torch.nn.Sequential(\n",
    "    torch.nn.LayerNorm(EMB_D),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(EMB_D, 64)\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "model_params = list(piece_tensors) + [turn_weights, queryW, keyW, valueW] + list(mlp_head.parameters())\n",
    "optimizer    = torch.optim.Adam(model_params, lr=LR)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 4.  Helper Functions\n",
    "# ──────────────────────────────────────────────\n",
    "piece_label_to_tensor = {i: piece_tensors[i] for i in range(15)}\n",
    "\n",
    "def board_to_tensor(board_ids):\n",
    "    return torch.stack([piece_label_to_tensor[i] for i in board_ids])\n",
    "\n",
    "def apply_turn_mask(board_ids, board_tensor, turn):\n",
    "    mask = [(1 <= p <= 7) if turn == \"white\" else (8 <= p <= 14) for p in board_ids]\n",
    "    if any(mask):\n",
    "        board_tensor = board_tensor.clone()\n",
    "        board_tensor[mask] = board_tensor[mask] @ turn_weights\n",
    "    return board_tensor\n",
    "\n",
    "def minmax_norm(t):\n",
    "    return (t - t.min()) / (t.max() - t.min() + 1e-6)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 5.  Training Loop\n",
    "# ──────────────────────────────────────────────\n",
    "def train_model(train_ds, epochs=5):\n",
    "    valid_cards = [c for c in train_ds if c[\"winner\"] == c[\"turn\"]]\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(valid_cards), BATCH_SZ):\n",
    "            batch = valid_cards[i:i+BATCH_SZ]\n",
    "            boards = []\n",
    "            targets = []\n",
    "\n",
    "            for card in batch:\n",
    "                turn = card[\"turn\"]\n",
    "                ids = card[\"input\"]\n",
    "                row, col = card[\"output\"]\n",
    "\n",
    "                bt = board_to_tensor(ids)\n",
    "                bt = apply_turn_mask(ids, bt, turn)\n",
    "                bt = minmax_norm(bt).to(device)\n",
    "\n",
    "                boards.append(bt)\n",
    "                targets.append(row * 64 + col)\n",
    "\n",
    "            boards  = torch.stack(boards).to(device)               # [B, 64, EMB_D]\n",
    "            targets = torch.tensor(targets, device=device)         # [B]\n",
    "\n",
    "            Q = minmax_norm(boards @ queryW)\n",
    "            K = minmax_norm(boards @ keyW)\n",
    "            V = boards @ valueW\n",
    "\n",
    "            Q = apply_rope(Q, rope_sin, rope_cos)\n",
    "            K = apply_rope(K, rope_sin, rope_cos)\n",
    "\n",
    "            # Normalize\n",
    "            Q = torch.nn.functional.layer_norm(Q, (EMB_D,))\n",
    "            K = torch.nn.functional.layer_norm(K, (EMB_D,))\n",
    "            V = torch.nn.functional.layer_norm(V, (EMB_D,))\n",
    "\n",
    "            attn_weights = torch.bmm(Q, K.transpose(1, 2)) / EMB_D**0.5\n",
    "            attn_out = torch.bmm(attn_weights.softmax(dim=-1), V)\n",
    "\n",
    "            logits_per_square = mlp_head(attn_out)      # [B, 64, 64]\n",
    "            logits = logits_per_square.view(boards.size(0), -1)  # [B, 4096]\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            total_loss += loss.item() * boards.size(0)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(valid_cards)\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_ds, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trial blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chess_move_transformer_split_bishops_hf.py\n",
    "import torch, random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset  # type hint\n",
    "\n",
    "# ---------------- Hyper‑params ----------------\n",
    "D_MODEL, N_HEADS, DEPTH = 1024, 8, 6\n",
    "FFN_MULT, BATCH_SZ, LR, EPOCHS = 4, 1111, 2e-4, 1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =============== Model definition ===============\n",
    "class ChessBlock(nn.Module):\n",
    "    def __init__(self, d=D_MODEL, heads=N_HEADS, ffn_mult=FFN_MULT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d)\n",
    "        self.attn = nn.MultiheadAttention(d, heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d, ffn_mult * d), nn.GELU(), nn.Linear(ffn_mult * d, d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChessMoveModel(nn.Module):\n",
    "    def __init__(self, d=D_MODEL, heads=N_HEADS, layers=DEPTH):\n",
    "        super().__init__()\n",
    "        self.piece_emb = nn.Embedding(15, d)\n",
    "        self.square_emb = nn.Embedding(64, d)\n",
    "        self.turn_weights = nn.Parameter(torch.randn(d, d))\n",
    "        self.blocks = nn.ModuleList([ChessBlock(d, heads) for _ in range(layers)])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d), nn.Linear(d, 64))\n",
    "\n",
    "    def forward(self, board_ids, turn_mask):          # board_ids [B,64]\n",
    "        sq = torch.arange(64, device=board_ids.device)\n",
    "        x = self.piece_emb(board_ids) + self.square_emb(sq)         # [B,64,d]\n",
    "        x = torch.where(turn_mask.unsqueeze(-1), x @ self.turn_weights, x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.head(x).view(x.size(0), -1)                     # [B,4096]\n",
    "\n",
    "\n",
    "# =============== Helper utilities ===============\n",
    "def make_turn_mask(board_batch: torch.Tensor, turns):\n",
    "    white_owned = (board_batch >= 1) & (board_batch <= 7)\n",
    "    black_owned = (board_batch >= 8)\n",
    "    masks = [\n",
    "        white_owned[i] if (t == \"white\" or t == 0) else black_owned[i]\n",
    "        for i, t in enumerate(turns)\n",
    "    ]\n",
    "    return torch.stack(masks)\n",
    "\n",
    "\n",
    "def hf_batch_iter(ds: Dataset, bs: int):\n",
    "    \"\"\"Iterate over a HF Dataset in shuffled mini‑batches.\"\"\"\n",
    "    ds = ds.shuffle(seed=random.randint(0, 1_000_000))\n",
    "    for start in range(0, len(ds), bs):\n",
    "        yield ds.select(range(start, min(start + bs, len(ds))))\n",
    "\n",
    "\n",
    "def list_batch_iter(data: List[Dict], bs: int):\n",
    "    random.shuffle(data)\n",
    "    for i in range(0, len(data), bs):\n",
    "        yield data[i : i + bs]\n",
    "\n",
    "\n",
    "# =============== Training loop ===============\n",
    "def train(model: nn.Module, dataset):\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    model.train()\n",
    "\n",
    "    # If it's a HF Dataset, ask it to output torch tensors for input/output\n",
    "    if isinstance(dataset, Dataset):\n",
    "        dataset = dataset.with_format(type=\"torch\", columns=[\"input\", \"output\"])\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total, correct, agg_loss = 0, 0, 0.0\n",
    "\n",
    "        batcher = hf_batch_iter(dataset, BATCH_SZ) if isinstance(dataset, Dataset) \\\n",
    "                  else list_batch_iter(dataset, BATCH_SZ)\n",
    "\n",
    "        pbar = tqdm.tqdm(batcher, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        for batch in pbar:\n",
    "            if isinstance(batch, Dataset):               # Hugging‑Face case\n",
    "                boards  = batch[\"input\"].to(DEVICE)      # already LongTensor [B,64]\n",
    "                outs    = batch[\"output\"].to(DEVICE)     # [B,2]\n",
    "                turns   = batch[\"turn\"]                  # still python list/ndarray\n",
    "            else:                                        # list-of-dicts case\n",
    "                boards  = torch.stack([b[\"input\"] for b in batch]).to(DEVICE)\n",
    "                outs    = torch.stack([b[\"output\"] for b in batch]).to(DEVICE)\n",
    "                turns   = [b[\"turn\"] for b in batch]\n",
    "\n",
    "            targets = (outs[:, 0] * 64 + outs[:, 1]).to(DEVICE)          # [B]\n",
    "            turn_mask = make_turn_mask(boards, turns).to(DEVICE)\n",
    "\n",
    "            logits = model(boards, turn_mask)\n",
    "            loss   = F.cross_entropy(logits, targets)\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            agg_loss += loss.item() * boards.size(0)\n",
    "            total    += boards.size(0)\n",
    "            correct  += (logits.argmax(dim=-1) == targets).sum().item()\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | loss {agg_loss/total:.4f} | acc {100*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChessMoveModel().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 981it [40:49,  2.45s/it, loss=3.5074]"
     ]
    }
   ],
   "source": [
    "train(model, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Key `mlp_head` is invalid, expected torch.Tensor but received <class 'torch.nn.modules.container.Sequential'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(piece_tensors):\n\u001b[0;32m     19\u001b[0m     state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpiece_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[1;32m---> 21\u001b[0m \u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchess_model_3k.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\safetensors\\torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\safetensors\\torch.py:470\u001b[0m, in \u001b[0;36m_flatten\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is invalid, expected torch.Tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    473\u001b[0m         invalid_tensors\u001b[38;5;241m.\u001b[39mappend(k)\n",
      "\u001b[1;31mValueError\u001b[0m: Key `mlp_head` is invalid, expected torch.Tensor but received <class 'torch.nn.modules.container.Sequential'>"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import save_file, save_model\n",
    "\n",
    "# Remove these lines (they cause shared memory issues)\n",
    "# \"E\": E, \"WR\": WR, ..., \"BB2\": BB2\n",
    "\n",
    "# Use only the indexed loop version:\n",
    "state_dict = {\n",
    "    \"turn_weights\": turn_weights,\n",
    "    \"queryW\": queryW,\n",
    "    \"keyW\": keyW,\n",
    "    \"valueW\": valueW,\n",
    "    \"rope_sin\": rope_sin,\n",
    "    \"rope_cos\": rope_cos,\n",
    "    \"mlp_head\": mlp_head\n",
    "    \n",
    "}\n",
    "\n",
    "for idx, tensor in enumerate(piece_tensors):\n",
    "    state_dict[f\"piece_{idx}\"] = tensor\n",
    "\n",
    "save_file(state_dict, \"chess_model_3k.safetensors\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
