{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "games = [{'Event': 'Rated Classical game',\n",
    " 'Site': 'https://lichess.org/j1dkb5dw',\n",
    " 'White': 'BFG9k',\n",
    " 'Black': 'mamalak',\n",
    " 'Result': '1-0',\n",
    " 'WhiteTitle': None,\n",
    " 'BlackTitle': None,\n",
    " 'WhiteElo': 1639,\n",
    " 'BlackElo': 1403,\n",
    " 'WhiteRatingDiff': 5,\n",
    " 'BlackRatingDiff': -8,\n",
    " 'UTCDate': datetime.date(2012, 12, 31),\n",
    " 'UTCTime': datetime.time(23, 1, 3),\n",
    " 'ECO': 'C00',\n",
    " 'Opening': 'French Defense: Normal Variation',\n",
    " 'Termination': 'Normal',\n",
    " 'TimeControl': '600+8',\n",
    " 'movetext': '1. e4 e6 2. d4 b6 3. a3 Bb7 4. Nc3 Nh6 5. Bxh6 gxh6 6. Be2 Qg5 7. Bg4 h5 8. Nf3 Qg6 9. Nh4 Qg5 10. Bxh5 Qxh4 11. Qf3 Kd8 12. Qxf7 Nc6 13. Qe8# 1-0'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "\n",
    "huggingface_hub.login(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 23/23 [00:23<00:00,  1.03s/files]\n",
      "Generating train split: 100%|██████████| 40885661/40885661 [00:26<00:00, 1551716.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = datasets.load_dataset('youngchiller40/chess-combined-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test (e.g., 90/10)\n",
    "split_dataset = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# legacy training block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 1.  Hyper‑params & device\n",
    "# ──────────────────────────────────────────────\n",
    "EMB_D    = 1024\n",
    "BATCH_SZ = 512\n",
    "LR       = 2e-4\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2.  Rotary Positional Embedding\n",
    "# ──────────────────────────────────────────────\n",
    "def build_rope_tables(seq_len: int, dim: int, device):\n",
    "    half = dim // 2\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(half, device=device) / half))\n",
    "    ang = torch.arange(seq_len, device=device).float().unsqueeze(1) * inv_freq[None, :]\n",
    "    return ang.sin(), ang.cos()\n",
    "\n",
    "rope_sin, rope_cos = build_rope_tables(64, EMB_D, device)\n",
    "rope_sin.requires_grad_(False)\n",
    "rope_cos.requires_grad_(False)\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    sin = sin.unsqueeze(0)\n",
    "    cos = cos.unsqueeze(0)\n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd  = x[..., 1::2]\n",
    "    rot_even = x_even * cos - x_odd * sin\n",
    "    rot_odd  = x_even * sin + x_odd * cos\n",
    "    x[..., 0::2] = rot_even\n",
    "    x[..., 1::2] = rot_odd\n",
    "    return x\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3.  Learnable Weights\n",
    "# ──────────────────────────────────────────────\n",
    "piece_tensors = torch.nn.ParameterList([\n",
    "    torch.nn.Parameter(torch.randn(EMB_D, device=device)) for _ in range(15)\n",
    "])\n",
    "turn_weights = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "queryW       = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "keyW         = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "valueW       = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "\n",
    "# MLP Head: outputs 64 logits per square\n",
    "mlp_head = torch.nn.Sequential(\n",
    "    torch.nn.LayerNorm(EMB_D),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(EMB_D, 64)\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "model_params = list(piece_tensors) + [turn_weights, queryW, keyW, valueW] + list(mlp_head.parameters())\n",
    "optimizer    = torch.optim.Adam(model_params, lr=LR)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 4.  Helper Functions\n",
    "# ──────────────────────────────────────────────\n",
    "piece_label_to_tensor = {i: piece_tensors[i] for i in range(15)}\n",
    "\n",
    "def board_to_tensor(board_ids):\n",
    "    return torch.stack([piece_label_to_tensor[i] for i in board_ids])\n",
    "\n",
    "def apply_turn_mask(board_ids, board_tensor, turn):\n",
    "    mask = [(1 <= p <= 7) if turn == \"white\" else (8 <= p <= 14) for p in board_ids]\n",
    "    if any(mask):\n",
    "        board_tensor = board_tensor.clone()\n",
    "        board_tensor[mask] = board_tensor[mask] @ turn_weights\n",
    "    return board_tensor\n",
    "\n",
    "def minmax_norm(t):\n",
    "    return (t - t.min()) / (t.max() - t.min() + 1e-6)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 5.  Training Loop\n",
    "# ──────────────────────────────────────────────\n",
    "def train_model(train_ds, epochs=5):\n",
    "    valid_cards = [c for c in train_ds if c[\"winner\"] == c[\"turn\"]]\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(valid_cards), BATCH_SZ):\n",
    "            batch = valid_cards[i:i+BATCH_SZ]\n",
    "            boards = []\n",
    "            targets = []\n",
    "\n",
    "            for card in batch:\n",
    "                turn = card[\"turn\"]\n",
    "                ids = card[\"input\"]\n",
    "                row, col = card[\"output\"]\n",
    "\n",
    "                bt = board_to_tensor(ids)\n",
    "                bt = apply_turn_mask(ids, bt, turn)\n",
    "                bt = minmax_norm(bt).to(device)\n",
    "\n",
    "                boards.append(bt)\n",
    "                targets.append(row * 64 + col)\n",
    "\n",
    "            boards  = torch.stack(boards).to(device)               # [B, 64, EMB_D]\n",
    "            targets = torch.tensor(targets, device=device)         # [B]\n",
    "\n",
    "            Q = minmax_norm(boards @ queryW)\n",
    "            K = minmax_norm(boards @ keyW)\n",
    "            V = boards @ valueW\n",
    "\n",
    "            Q = apply_rope(Q, rope_sin, rope_cos)\n",
    "            K = apply_rope(K, rope_sin, rope_cos)\n",
    "\n",
    "            # Normalize\n",
    "            Q = torch.nn.functional.layer_norm(Q, (EMB_D,))\n",
    "            K = torch.nn.functional.layer_norm(K, (EMB_D,))\n",
    "            V = torch.nn.functional.layer_norm(V, (EMB_D,))\n",
    "\n",
    "            attn_weights = torch.bmm(Q, K.transpose(1, 2)) / EMB_D**0.5\n",
    "            attn_out = torch.bmm(attn_weights.softmax(dim=-1), V)\n",
    "\n",
    "            logits_per_square = mlp_head(attn_out)      # [B, 64, 64]\n",
    "            logits = logits_per_square.view(boards.size(0), -1)  # [B, 4096]\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            total_loss += loss.item() * boards.size(0)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(valid_cards)\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_ds, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trial blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://huggingface.co/youngchiller40/chess-move-transformer/resolve/main/chess_move_model.pt\" to /root/.cache/torch/hub/checkpoints/chess_move_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293M/293M [00:07<00:00, 41.5MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChessMoveModel(\n",
       "  (piece_emb): Embedding(15, 1024)\n",
       "  (square_emb): Embedding(64, 1024)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x ChessBlock(\n",
       "      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = ChessMoveModel()\n",
    "model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
    "    'https://huggingface.co/youngchiller40/chess-move-transformer/resolve/main/chess_move_model.pt',\n",
    "    map_location=\"cpu\"\n",
    "))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chess_move_transformer_split_bishops_hf.py\n",
    "import torch, random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset  # type hint\n",
    "import tqdm\n",
    "\n",
    "# ---------------- Hyper‑params ----------------\n",
    "D_MODEL, N_HEADS, DEPTH = 1024, 8, 6\n",
    "FFN_MULT, BATCH_SZ, LR, EPOCHS = 4, 1111, 2e-4, 1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =============== Model definition ===============\n",
    "class ChessBlock(nn.Module):\n",
    "    def __init__(self, d=D_MODEL, heads=N_HEADS, ffn_mult=FFN_MULT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d)\n",
    "        self.attn = nn.MultiheadAttention(d, heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d, ffn_mult * d), nn.GELU(), nn.Linear(ffn_mult * d, d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChessMoveModel(nn.Module):\n",
    "    def __init__(self, d=D_MODEL, heads=N_HEADS, layers=DEPTH):\n",
    "        super().__init__()\n",
    "        self.piece_emb = nn.Embedding(15, d)\n",
    "        self.square_emb = nn.Embedding(64, d)\n",
    "        self.turn_weights = nn.Parameter(torch.randn(d, d))\n",
    "        self.blocks = nn.ModuleList([ChessBlock(d, heads) for _ in range(layers)])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d), nn.Linear(d, 64))\n",
    "\n",
    "    def forward(self, board_ids, turn_mask):          # board_ids [B,64]\n",
    "        sq = torch.arange(64, device=board_ids.device)\n",
    "        x = self.piece_emb(board_ids) + self.square_emb(sq)         # [B,64,d]\n",
    "        x = torch.where(turn_mask.unsqueeze(-1), x @ self.turn_weights, x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.head(x).view(x.size(0), -1)                     # [B,4096]\n",
    "\n",
    "\n",
    "# =============== Helper utilities ===============\n",
    "def make_turn_mask(board_batch: torch.Tensor, turns):\n",
    "    white_owned = (board_batch >= 1) & (board_batch <= 7)\n",
    "    black_owned = (board_batch >= 8)\n",
    "    masks = [\n",
    "        white_owned[i] if (t == \"white\" or t == 0) else black_owned[i]\n",
    "        for i, t in enumerate(turns)\n",
    "    ]\n",
    "    return torch.stack(masks)\n",
    "\n",
    "\n",
    "def hf_batch_iter(ds: Dataset, bs: int):\n",
    "    \"\"\"Iterate over a HF Dataset in shuffled mini‑batches.\"\"\"\n",
    "    ds = ds.shuffle(seed=random.randint(0, 1_000_000))\n",
    "    for start in range(0, len(ds), bs):\n",
    "        yield ds.select(range(start, min(start + bs, len(ds))))\n",
    "\n",
    "\n",
    "def list_batch_iter(data: List[Dict], bs: int):\n",
    "    random.shuffle(data)\n",
    "    for i in range(0, len(data), bs):\n",
    "        yield data[i : i + bs]\n",
    "\n",
    "\n",
    "# =============== Training loop ===============\n",
    "def train(model: nn.Module, dataset):\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    model.train()\n",
    "\n",
    "    # If it's a HF Dataset, ask it to output torch tensors for input/output\n",
    "    if isinstance(dataset, Dataset):\n",
    "        dataset = dataset.with_format(type=\"torch\", columns=[\"input\", \"output\"])\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total, correct, agg_loss = 0, 0, 0.0\n",
    "\n",
    "        batcher = hf_batch_iter(dataset, BATCH_SZ) if isinstance(dataset, Dataset) \\\n",
    "                  else list_batch_iter(dataset, BATCH_SZ)\n",
    "\n",
    "        total_batches = len(dataset) // BATCH_SZ\n",
    "        pbar = tqdm.tqdm(batcher, desc=f\"Epoch {epoch}/{EPOCHS}\", total=total_batches)\n",
    "        for batch_num, batch in enumerate(pbar):\n",
    "            if isinstance(batch, Dataset):               # Hugging‑Face case\n",
    "                boards  = batch[\"input\"].to(DEVICE)      # already LongTensor [B,64]\n",
    "                outs    = batch[\"output\"].to(DEVICE)     # [B,2]\n",
    "                turns   = batch[\"turn\"]                  # still python list/ndarray\n",
    "            else:                                        # list-of-dicts case\n",
    "                boards  = torch.stack([b[\"input\"] for b in batch]).to(DEVICE)\n",
    "                outs    = torch.stack([b[\"output\"] for b in batch]).to(DEVICE)\n",
    "                turns   = [b[\"turn\"] for b in batch]\n",
    "\n",
    "            targets = (outs[:, 0] * 64 + outs[:, 1]).to(DEVICE)          # [B]\n",
    "            turn_mask = make_turn_mask(boards, turns).to(DEVICE)\n",
    "\n",
    "            logits = model(boards, turn_mask)\n",
    "            loss   = F.cross_entropy(logits, targets)\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            agg_loss += loss.item() * boards.size(0)\n",
    "            total    += boards.size(0)\n",
    "            correct  += (logits.argmax(dim=-1) == targets).sum().item()\n",
    "            \n",
    "            # Update progress bar with current loss and percentage complete\n",
    "            percent_complete = 100 * (batch_num + 1) / total_batches\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'complete': f'{percent_complete:.1f}%'})\n",
    "\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | loss {agg_loss/total:.4f} | acc {100*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChessMoveModel(\n",
       "  (piece_emb): Embedding(15, 1024)\n",
       "  (square_emb): Embedding(64, 1024)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x ChessBlock(\n",
       "      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  20%|██        | 6019/29440 [4:10:25<16:14:26,  2.50s/it, loss=2.7011, complete=20.4%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 111\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m    108\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    109\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 111\u001b[0m agg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m boards\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    112\u001b[0m total    \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m boards\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    113\u001b[0m correct  \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m targets)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights (recommended)\n",
    "torch.save(model.state_dict(), \"chess_move_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"d_model\": D_MODEL,\n",
    "    \"n_heads\": N_HEADS,\n",
    "    \"depth\": DEPTH,\n",
    "    \"ffn_mult\": FFN_MULT\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(model_config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chess_move_model.pt: 100%|██████████| 307M/307M [00:11<00:00, 26.3MB/s] \n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/youngchiller40/chess-move-transformer/commit/d38c786a7604c0c595fd6f10eaf320d04cb2cc2f', commit_message='Upload config.json with huggingface_hub', commit_description='', oid='d38c786a7604c0c595fd6f10eaf320d04cb2cc2f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/youngchiller40/chess-move-transformer', endpoint='https://huggingface.co', repo_type='model', repo_id='youngchiller40/chess-move-transformer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, upload_file\n",
    "\n",
    "# Upload model weights\n",
    "upload_file(\n",
    "    path_or_fileobj=\"chess_move_model.pt\",\n",
    "    path_in_repo=\"chess_move_model.pt\",\n",
    "    repo_id=\"youngchiller40/chess-move-transformer\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "# Upload config\n",
    "upload_file(\n",
    "    path_or_fileobj=\"config.json\",\n",
    "    path_in_repo=\"config.json\",\n",
    "    repo_id=\"youngchiller40/chess-move-transformer\",\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
