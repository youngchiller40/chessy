{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load using streaming mode\n",
    "dataset = load_dataset(\"Lichess/standard-chess-games\", split=\"train\", streaming=True)\n",
    "\n",
    "# Get a small sample of 10 games\n",
    "sample = []\n",
    "for i, row in enumerate(dataset):\n",
    "    sample.append(row)\n",
    "    if i == 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WP = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "\n",
    "WR = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "\n",
    "WN = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "\n",
    "WB1 = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "WB2 = torch.randn(1000, requires_grad=True, device='cuda') \n",
    "\n",
    "WQ = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "WK = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "\n",
    "BP = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "\n",
    "BR = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "\n",
    "BN = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "\n",
    "BB1 = torch.randn(1000, requires_grad=True, device='cuda') \n",
    "BB2 = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "\n",
    "BQ = torch.randn(1000, requires_grad=True, device='cuda')\n",
    "BK = torch.randn(1000, requires_grad=True, device='cuda')   \n",
    "\n",
    "E = torch.randn(1000, requires_grad=True, device='cuda')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_weights = torch.randn([1000, 1000], requires_grad=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryW = torch.randn([1000, 1000], requires_grad=True, device='cuda')\n",
    "keyW = torch.randn([1000, 1000], requires_grad=True, device='cuda')\n",
    "valueW = torch.randn([1000, 64], requires_grad=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_emb = (query_embeddings @ key_embeddings.T) + value_embeddings\n",
    "# final_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "games = [{'Event': 'Rated Classical game',\n",
    " 'Site': 'https://lichess.org/j1dkb5dw',\n",
    " 'White': 'BFG9k',\n",
    " 'Black': 'mamalak',\n",
    " 'Result': '1-0',\n",
    " 'WhiteTitle': None,\n",
    " 'BlackTitle': None,\n",
    " 'WhiteElo': 1639,\n",
    " 'BlackElo': 1403,\n",
    " 'WhiteRatingDiff': 5,\n",
    " 'BlackRatingDiff': -8,\n",
    " 'UTCDate': datetime.date(2012, 12, 31),\n",
    " 'UTCTime': datetime.time(23, 1, 3),\n",
    " 'ECO': 'C00',\n",
    " 'Opening': 'French Defense: Normal Variation',\n",
    " 'Termination': 'Normal',\n",
    " 'TimeControl': '600+8',\n",
    " 'movetext': '1. e4 e6 2. d4 b6 3. a3 Bb7 4. Nc3 Nh6 5. Bxh6 gxh6 6. Be2 Qg5 7. Bg4 h5 8. Nf3 Qg6 9. Nh4 Qg5 10. Bxh5 Qxh4 11. Qf3 Kd8 12. Qxf7 Nc6 13. Qe8# 1-0'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import chess\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------- CONFIG: integer IDs for piece types ------------------- #\n",
    "piece_label_to_id = {\n",
    "    \"WR\": 1, \"WN\": 2, \"WB1\": 3, \"WQ\": 4, \"WK\": 5, \"WB2\": 6, \"WP\": 7,\n",
    "    \"BP\": 8, \"BR\": 9, \"BN\": 10, \"BB1\": 11, \"BQ\": 12, \"BK\": 13, \"BB2\": 14,\n",
    "}\n",
    "\n",
    "# ------------------- INITIAL BOARD ------------------- #\n",
    "initial_map = {\n",
    "    0: \"WR\", 1: \"WN\", 2: \"WB1\", 3: \"WQ\", 4: \"WK\", 5: \"WB2\", 6: \"WN\", 7: \"WR\",\n",
    "    8: \"WP\", 9: \"WP\", 10: \"WP\", 11: \"WP\", 12: \"WP\", 13: \"WP\", 14: \"WP\", 15: \"WP\",\n",
    "    48: \"BP\", 49: \"BP\", 50: \"BP\", 51: \"BP\", 52: \"BP\", 53: \"BP\", 54: \"BP\", 55: \"BP\",\n",
    "    56: \"BR\", 57: \"BN\", 58: \"BB1\", 59: \"BQ\", 60: \"BK\", 61: \"BB2\", 62: \"BN\", 63: \"BR\",\n",
    "}\n",
    "\n",
    "# ------------------- Move Parser ------------------- #\n",
    "_move_num = re.compile(r\"^\\d+\\.(\\.\\.)?$\")\n",
    "\n",
    "def san_stream(movetext: str):\n",
    "    for tok in movetext.replace(\"\\n\", \" \").split():\n",
    "        if _move_num.match(tok) or tok in {\"1-0\", \"0-1\", \"1/2-1/2\", \"*\"}:\n",
    "            continue\n",
    "        yield tok\n",
    "\n",
    "# ------------------- Convert mapping to tensor ------------------- #\n",
    "def mapping_to_tensor(mapping: dict) -> torch.LongTensor:\n",
    "    return torch.tensor([\n",
    "        piece_label_to_id.get(mapping.get(i), 0) for i in range(64)\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "# ------------------- Determine winner string ------------------- #\n",
    "def result_to_winner(result: str) -> str:\n",
    "    if result == \"1-0\":\n",
    "        return \"white\"\n",
    "    elif result == \"0-1\":\n",
    "        return \"black\"\n",
    "    else:\n",
    "        return \"draw\"\n",
    "    \n",
    "    \n",
    "def create_dataset_from_games(game_dicts: list[dict]) -> list[dict]:\n",
    "    full_dataset = []\n",
    "\n",
    "    for game in game_dicts:\n",
    "        try:\n",
    "            board = chess.Board()\n",
    "            mapping = initial_map.copy()\n",
    "            states = [mapping_to_tensor(mapping)]\n",
    "            turns = [\"white\"]  # starting with white\n",
    "            move_vectors = []\n",
    "\n",
    "            for san in san_stream(game[\"movetext\"]):\n",
    "                move = board.parse_san(san)\n",
    "                from_sq, to_sq = move.from_square, move.to_square\n",
    "                move_vector = [from_sq, to_sq]\n",
    "\n",
    "                # Update mapping\n",
    "                moving_id = mapping.pop(from_sq, None)\n",
    "\n",
    "                if to_sq in mapping:\n",
    "                    mapping.pop(to_sq)\n",
    "\n",
    "                if board.is_en_passant(move):\n",
    "                    ep_target = to_sq + (-8 if board.turn else 8)\n",
    "                    mapping.pop(ep_target, None)\n",
    "\n",
    "                mapping[to_sq] = moving_id\n",
    "\n",
    "                # Skip rook move, just rely on king's move in castling\n",
    "                board.push(move)\n",
    "\n",
    "                states.append(mapping_to_tensor(mapping))\n",
    "                turns.append(\"white\" if board.turn else \"black\")\n",
    "                move_vectors.append(move_vector)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        winner = result_to_winner(game[\"Result\"])\n",
    "\n",
    "        for i in range(len(states) - 1):\n",
    "            full_dataset.append({\n",
    "                \"input\": states[i],\n",
    "                \"output\": torch.tensor(move_vectors[i], dtype=torch.long),  # output is [from_sq, to_sq]\n",
    "                \"turn\": turns[i],\n",
    "                \"winner\": winner\n",
    "            })\n",
    "\n",
    "    return full_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AWright\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "\n",
    "huggingface_hub.login(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('youngchiller40/chessset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test (e.g., 90/10)\n",
    "split_dataset = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 1.  Hyper‑params & device\n",
    "# ──────────────────────────────────────────────\n",
    "EMB_D    = 1024\n",
    "BATCH_SZ = 512\n",
    "LR       = 2e-4\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2.  Rotary Positional Embedding\n",
    "# ──────────────────────────────────────────────\n",
    "def build_rope_tables(seq_len: int, dim: int, device):\n",
    "    half = dim // 2\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(half, device=device) / half))\n",
    "    ang = torch.arange(seq_len, device=device).float().unsqueeze(1) * inv_freq[None, :]\n",
    "    return ang.sin(), ang.cos()\n",
    "\n",
    "rope_sin, rope_cos = build_rope_tables(64, EMB_D, device)\n",
    "rope_sin.requires_grad_(False)\n",
    "rope_cos.requires_grad_(False)\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    sin = sin.unsqueeze(0)\n",
    "    cos = cos.unsqueeze(0)\n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd  = x[..., 1::2]\n",
    "    rot_even = x_even * cos - x_odd * sin\n",
    "    rot_odd  = x_even * sin + x_odd * cos\n",
    "    x[..., 0::2] = rot_even\n",
    "    x[..., 1::2] = rot_odd\n",
    "    return x\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3.  Learnable Weights\n",
    "# ──────────────────────────────────────────────\n",
    "piece_tensors = torch.nn.ParameterList([\n",
    "    torch.nn.Parameter(torch.randn(EMB_D, device=device)) for _ in range(15)\n",
    "])\n",
    "turn_weights = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "queryW       = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "keyW         = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "valueW       = torch.nn.Parameter(torch.randn(EMB_D, EMB_D, device=device))\n",
    "\n",
    "# MLP Head: outputs 64 logits per square\n",
    "mlp_head = torch.nn.Sequential(\n",
    "    torch.nn.LayerNorm(EMB_D),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(EMB_D, 64)\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "model_params = list(piece_tensors) + [turn_weights, queryW, keyW, valueW] + list(mlp_head.parameters())\n",
    "optimizer    = torch.optim.Adam(model_params, lr=LR)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 4.  Helper Functions\n",
    "# ──────────────────────────────────────────────\n",
    "piece_label_to_tensor = {i: piece_tensors[i] for i in range(15)}\n",
    "\n",
    "def board_to_tensor(board_ids):\n",
    "    return torch.stack([piece_label_to_tensor[i] for i in board_ids])\n",
    "\n",
    "def apply_turn_mask(board_ids, board_tensor, turn):\n",
    "    mask = [(1 <= p <= 7) if turn == \"white\" else (8 <= p <= 14) for p in board_ids]\n",
    "    if any(mask):\n",
    "        board_tensor = board_tensor.clone()\n",
    "        board_tensor[mask] = board_tensor[mask] @ turn_weights\n",
    "    return board_tensor\n",
    "\n",
    "def minmax_norm(t):\n",
    "    return (t - t.min()) / (t.max() - t.min() + 1e-6)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 5.  Training Loop\n",
    "# ──────────────────────────────────────────────\n",
    "def train_model(train_ds, epochs=5):\n",
    "    valid_cards = [c for c in train_ds if c[\"winner\"] == c[\"turn\"]]\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(valid_cards), BATCH_SZ):\n",
    "            batch = valid_cards[i:i+BATCH_SZ]\n",
    "            boards = []\n",
    "            targets = []\n",
    "\n",
    "            for card in batch:\n",
    "                turn = card[\"turn\"]\n",
    "                ids = card[\"input\"]\n",
    "                row, col = card[\"output\"]\n",
    "\n",
    "                bt = board_to_tensor(ids)\n",
    "                bt = apply_turn_mask(ids, bt, turn)\n",
    "                bt = minmax_norm(bt).to(device)\n",
    "\n",
    "                boards.append(bt)\n",
    "                targets.append(row * 64 + col)\n",
    "\n",
    "            boards  = torch.stack(boards).to(device)               # [B, 64, EMB_D]\n",
    "            targets = torch.tensor(targets, device=device)         # [B]\n",
    "\n",
    "            Q = minmax_norm(boards @ queryW)\n",
    "            K = minmax_norm(boards @ keyW)\n",
    "            V = boards @ valueW\n",
    "\n",
    "            Q = apply_rope(Q, rope_sin, rope_cos)\n",
    "            K = apply_rope(K, rope_sin, rope_cos)\n",
    "\n",
    "            # Normalize\n",
    "            Q = torch.nn.functional.layer_norm(Q, (EMB_D,))\n",
    "            K = torch.nn.functional.layer_norm(K, (EMB_D,))\n",
    "            V = torch.nn.functional.layer_norm(V, (EMB_D,))\n",
    "\n",
    "            attn_weights = torch.bmm(Q, K.transpose(1, 2)) / EMB_D**0.5\n",
    "            attn_out = torch.bmm(attn_weights.softmax(dim=-1), V)\n",
    "\n",
    "            logits_per_square = mlp_head(attn_out)      # [B, 64, 64]\n",
    "            logits = logits_per_square.view(boards.size(0), -1)  # [B, 4096]\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            total_loss += loss.item() * boards.size(0)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(valid_cards)\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.3911\n",
      "Loss: 8.3788\n",
      "Loss: 8.3576\n",
      "Loss: 8.3013\n",
      "Loss: 8.2600\n",
      "Loss: 8.2483\n",
      "Loss: 8.2566\n",
      "Loss: 8.1601\n",
      "Loss: 8.2021\n",
      "Loss: 8.1980\n",
      "Loss: 8.1554\n",
      "Loss: 8.1453\n",
      "Loss: 8.1204\n",
      "Loss: 8.1052\n",
      "Loss: 8.2068\n",
      "Loss: 8.1087\n",
      "Loss: 8.1614\n",
      "Loss: 8.1249\n",
      "Loss: 8.1497\n",
      "Loss: 8.0826\n",
      "Loss: 8.1276\n",
      "Loss: 8.1154\n",
      "Loss: 8.1464\n",
      "Loss: 8.1264\n",
      "Loss: 8.1286\n",
      "Loss: 8.0979\n",
      "Loss: 8.0896\n",
      "Loss: 8.1569\n",
      "Loss: 8.0828\n",
      "Loss: 8.0853\n",
      "Loss: 8.0954\n",
      "Loss: 8.0041\n",
      "Loss: 8.0696\n",
      "Loss: 8.0422\n",
      "Loss: 8.1046\n",
      "Loss: 8.0586\n",
      "Loss: 8.0431\n",
      "Loss: 8.0100\n",
      "Loss: 8.0700\n",
      "Loss: 8.0167\n",
      "Loss: 8.0161\n",
      "Loss: 8.0215\n",
      "Loss: 8.0480\n",
      "Loss: 8.0402\n",
      "Loss: 8.0179\n",
      "Loss: 8.0089\n",
      "Loss: 8.0258\n",
      "Loss: 8.0087\n",
      "Loss: 7.9814\n",
      "Loss: 8.0021\n",
      "Loss: 7.9700\n",
      "Loss: 7.9736\n",
      "Loss: 7.9737\n",
      "Loss: 7.8579\n",
      "Loss: 7.9301\n",
      "Loss: 7.9264\n",
      "Loss: 7.9307\n",
      "Loss: 7.9264\n",
      "Loss: 7.8791\n",
      "Loss: 7.9198\n",
      "Loss: 7.9122\n",
      "Loss: 7.8644\n",
      "Loss: 7.8889\n",
      "Loss: 7.8929\n",
      "Loss: 7.8948\n",
      "Loss: 7.8834\n",
      "Loss: 7.8417\n",
      "Loss: 7.8200\n",
      "Loss: 7.8103\n",
      "Loss: 7.8478\n",
      "Loss: 7.8485\n",
      "Loss: 7.8144\n",
      "Loss: 7.7908\n",
      "Loss: 7.8188\n",
      "Loss: 7.7358\n",
      "Loss: 7.7207\n",
      "Loss: 7.7730\n",
      "Loss: 7.7552\n",
      "Loss: 7.7126\n",
      "Loss: 7.7076\n",
      "Loss: 7.6874\n",
      "Loss: 7.7761\n",
      "Loss: 7.7483\n",
      "Loss: 7.6714\n",
      "Loss: 7.6437\n",
      "Loss: 7.6653\n",
      "Loss: 7.6592\n",
      "Loss: 7.5961\n",
      "Loss: 7.5998\n",
      "Loss: 7.5538\n",
      "Loss: 7.6147\n",
      "Loss: 7.5089\n",
      "Loss: 7.5541\n",
      "Loss: 7.5270\n",
      "Loss: 7.4727\n",
      "Loss: 7.5166\n",
      "Loss: 7.3777\n",
      "Loss: 7.5259\n",
      "Loss: 7.3925\n",
      "Loss: 7.4446\n",
      "Loss: 7.4620\n",
      "Loss: 7.3182\n",
      "Loss: 7.3371\n",
      "Loss: 7.3633\n",
      "Loss: 7.3265\n",
      "Loss: 7.2451\n",
      "Loss: 7.3254\n",
      "Loss: 7.2551\n",
      "Loss: 7.2046\n",
      "Loss: 7.2826\n",
      "Loss: 7.2247\n",
      "Loss: 7.1963\n",
      "Loss: 7.1415\n",
      "Loss: 7.1840\n",
      "Loss: 7.1048\n",
      "Loss: 7.0872\n",
      "Loss: 6.9372\n",
      "Loss: 7.0481\n",
      "Loss: 7.0243\n",
      "Loss: 7.0538\n",
      "Loss: 6.9693\n",
      "Loss: 6.9723\n",
      "Loss: 6.8956\n",
      "Loss: 6.9290\n",
      "Loss: 6.8860\n",
      "Loss: 6.8941\n",
      "Loss: 6.7882\n",
      "Loss: 6.7700\n",
      "Loss: 6.8007\n",
      "Loss: 6.7654\n",
      "Loss: 6.7475\n",
      "Loss: 6.7156\n",
      "Loss: 6.7940\n",
      "Loss: 6.8058\n",
      "Loss: 6.6864\n",
      "Loss: 6.6499\n",
      "Loss: 6.6729\n",
      "Loss: 6.6382\n",
      "Loss: 6.7265\n",
      "Loss: 6.6140\n",
      "Loss: 6.5332\n",
      "Loss: 6.5441\n",
      "Loss: 6.5089\n",
      "Loss: 6.5790\n",
      "Loss: 6.5420\n",
      "Loss: 6.4590\n",
      "Loss: 6.4355\n",
      "Loss: 6.4956\n",
      "Loss: 6.4086\n",
      "Loss: 6.3425\n",
      "Loss: 6.4164\n",
      "Loss: 6.3879\n",
      "Loss: 6.3741\n",
      "Loss: 6.3822\n",
      "Loss: 6.2982\n",
      "Loss: 6.2665\n",
      "Loss: 6.3490\n",
      "Loss: 6.3624\n",
      "Loss: 6.3019\n",
      "Loss: 6.2570\n",
      "Loss: 6.3582\n",
      "Loss: 6.2470\n",
      "Loss: 6.1788\n",
      "Loss: 6.2312\n",
      "Loss: 6.1421\n",
      "Loss: 6.1237\n",
      "Loss: 6.2203\n",
      "Loss: 6.1651\n",
      "Loss: 6.2617\n",
      "Loss: 6.1749\n",
      "Loss: 6.0811\n",
      "Loss: 6.1984\n",
      "Loss: 6.2127\n",
      "Loss: 6.2271\n",
      "Loss: 6.1841\n",
      "Loss: 6.1623\n",
      "Loss: 6.1460\n",
      "Loss: 6.0370\n",
      "Loss: 6.1909\n",
      "Loss: 6.1387\n",
      "Loss: 6.0974\n",
      "Loss: 6.0850\n",
      "Loss: 6.1024\n",
      "Loss: 5.9857\n",
      "Loss: 6.1415\n",
      "Loss: 6.1617\n",
      "Loss: 6.0857\n",
      "Loss: 6.0172\n",
      "Loss: 6.1460\n",
      "Loss: 6.1649\n",
      "Loss: 6.2008\n",
      "Loss: 6.1808\n",
      "Loss: 6.1503\n",
      "Loss: 6.1315\n",
      "Loss: 6.1571\n",
      "Loss: 6.1471\n",
      "Loss: 6.0471\n",
      "Loss: 6.1360\n",
      "Loss: 6.1465\n",
      "Loss: 6.0944\n",
      "Loss: 6.0489\n",
      "Loss: 6.1476\n",
      "Loss: 6.0047\n",
      "Loss: 6.0170\n",
      "Loss: 6.0137\n",
      "Loss: 6.1057\n",
      "Loss: 6.0347\n",
      "Loss: 6.0154\n",
      "Loss: 5.9870\n",
      "Loss: 6.0862\n",
      "Loss: 6.2347\n",
      "Loss: 6.0849\n",
      "Loss: 6.0681\n",
      "Loss: 6.0577\n",
      "Loss: 6.0674\n",
      "Loss: 6.0026\n",
      "Loss: 6.1188\n",
      "Loss: 5.9680\n",
      "Loss: 6.1900\n",
      "Loss: 6.0811\n",
      "Loss: 6.0915\n",
      "Loss: 6.0149\n",
      "Loss: 6.1875\n",
      "Loss: 6.1803\n",
      "Loss: 6.1303\n",
      "Loss: 6.0106\n",
      "Loss: 6.0063\n",
      "Loss: 6.1944\n",
      "Loss: 6.0683\n",
      "Loss: 6.1566\n",
      "Loss: 6.1677\n",
      "Loss: 6.0738\n",
      "Loss: 6.0181\n",
      "Loss: 6.1149\n",
      "Loss: 6.0129\n",
      "Loss: 6.1885\n",
      "Loss: 6.0465\n",
      "Loss: 6.1681\n",
      "Loss: 6.1064\n",
      "Loss: 6.1181\n",
      "Loss: 6.0016\n",
      "Loss: 5.9512\n",
      "Loss: 6.0441\n",
      "Loss: 6.0726\n",
      "Loss: 6.0938\n",
      "Loss: 6.0258\n",
      "Loss: 6.1957\n",
      "Loss: 6.0171\n",
      "Loss: 6.1362\n",
      "Loss: 6.0363\n",
      "Loss: 6.0433\n",
      "Loss: 5.9528\n",
      "Loss: 6.0029\n",
      "Loss: 6.0608\n",
      "Loss: 6.0014\n",
      "Loss: 6.0660\n",
      "Loss: 5.9851\n",
      "Loss: 6.0583\n",
      "Loss: 6.0370\n",
      "Loss: 6.0224\n",
      "Loss: 5.9566\n",
      "Loss: 6.0622\n",
      "Loss: 6.0312\n",
      "Loss: 5.9749\n",
      "Loss: 5.9585\n",
      "Loss: 5.9647\n",
      "Loss: 6.1237\n",
      "Loss: 6.0404\n",
      "Loss: 5.9757\n",
      "Loss: 6.0485\n",
      "Loss: 6.0920\n",
      "Loss: 6.0877\n",
      "Loss: 6.0634\n",
      "Loss: 6.0058\n",
      "Loss: 6.0802\n",
      "Loss: 5.9468\n",
      "Loss: 5.9366\n",
      "Loss: 5.9609\n",
      "Loss: 5.9038\n",
      "Loss: 6.0421\n",
      "Loss: 5.9031\n",
      "Loss: 6.0198\n",
      "Loss: 6.0933\n",
      "Loss: 5.9256\n",
      "Loss: 5.9556\n",
      "Loss: 6.1850\n",
      "Loss: 6.0930\n",
      "Loss: 6.0648\n",
      "Loss: 6.0520\n",
      "Loss: 6.0207\n",
      "Loss: 6.1116\n",
      "Loss: 6.1791\n",
      "Loss: 6.0375\n",
      "Loss: 6.2376\n",
      "Loss: 5.9204\n",
      "Loss: 5.9896\n",
      "Loss: 6.0077\n",
      "Loss: 5.9117\n",
      "Loss: 5.9604\n",
      "Loss: 6.0199\n",
      "Loss: 5.9415\n",
      "Loss: 5.9824\n",
      "Loss: 5.9777\n",
      "Loss: 5.9954\n",
      "Loss: 6.0508\n",
      "Loss: 5.9441\n",
      "Loss: 5.9024\n",
      "Loss: 5.9173\n",
      "Loss: 5.8701\n",
      "Loss: 6.0095\n",
      "Loss: 5.8849\n",
      "Loss: 5.9415\n",
      "Loss: 5.9460\n",
      "Loss: 5.8514\n",
      "Loss: 5.9724\n",
      "Loss: 5.8653\n",
      "Loss: 6.0562\n",
      "Loss: 5.9304\n",
      "Loss: 5.9908\n",
      "Loss: 5.9884\n",
      "Loss: 5.9830\n",
      "Loss: 6.0001\n",
      "Loss: 6.0469\n",
      "Loss: 5.9721\n",
      "Loss: 5.9817\n",
      "Loss: 5.8905\n",
      "Loss: 6.0084\n",
      "Loss: 5.8845\n",
      "Loss: 6.0389\n",
      "Loss: 5.9608\n",
      "Loss: 5.8694\n",
      "Loss: 5.9689\n",
      "Loss: 6.0308\n",
      "Loss: 5.9619\n",
      "Loss: 5.8608\n",
      "Loss: 5.8846\n",
      "Loss: 5.9295\n",
      "Loss: 6.0482\n",
      "Loss: 5.9736\n",
      "Loss: 5.9024\n",
      "Loss: 6.0371\n",
      "Loss: 5.9205\n",
      "Loss: 6.0929\n",
      "Loss: 6.0350\n",
      "Loss: 5.9911\n",
      "Loss: 6.0234\n",
      "Loss: 5.8735\n",
      "Loss: 5.9848\n",
      "Loss: 6.1131\n",
      "Loss: 5.9166\n",
      "Loss: 5.8420\n",
      "Loss: 5.9793\n",
      "Loss: 5.9036\n",
      "Loss: 6.0288\n",
      "Loss: 6.0417\n",
      "Loss: 5.9220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 127\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_ds, epochs)\u001b[0m\n\u001b[0;32m    124\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m boards\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 127\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(train_ds, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trial block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chess_move_transformer_split_bishops_hf.py\n",
    "import torch, random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset  # type hint\n",
    "\n",
    "# ---------------- Hyper‑params ----------------\n",
    "D_MODEL, N_HEADS, DEPTH = 1024, 8, 6\n",
    "FFN_MULT, BATCH_SZ, LR, EPOCHS = 4, 4, 2e-4, 1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# =============== Model definition ===============\n",
    "class ChessBlock(nn.Module):\n",
    "    def __init__(self, d=D_MODEL, heads=N_HEADS, ffn_mult=FFN_MULT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d)\n",
    "        self.attn = nn.MultiheadAttention(d, heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d, ffn_mult * d), nn.GELU(), nn.Linear(ffn_mult * d, d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChessMoveModel(nn.Module):\n",
    "    def __init__(self, d=D_MODEL, heads=N_HEADS, layers=DEPTH):\n",
    "        super().__init__()\n",
    "        self.piece_emb = nn.Embedding(15, d)\n",
    "        self.square_emb = nn.Embedding(64, d)\n",
    "        self.turn_weights = nn.Parameter(torch.randn(d, d))\n",
    "        self.blocks = nn.ModuleList([ChessBlock(d, heads) for _ in range(layers)])\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d), nn.Linear(d, 64))\n",
    "\n",
    "    def forward(self, board_ids, turn_mask):          # board_ids [B,64]\n",
    "        sq = torch.arange(64, device=board_ids.device)\n",
    "        x = self.piece_emb(board_ids) + self.square_emb(sq)         # [B,64,d]\n",
    "        x = torch.where(turn_mask.unsqueeze(-1), x @ self.turn_weights, x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.head(x).view(x.size(0), -1)                     # [B,4096]\n",
    "\n",
    "\n",
    "# =============== Helper utilities ===============\n",
    "def make_turn_mask(board_batch: torch.Tensor, turns):\n",
    "    white_owned = (board_batch >= 1) & (board_batch <= 7)\n",
    "    black_owned = (board_batch >= 8)\n",
    "    masks = [\n",
    "        white_owned[i] if (t == \"white\" or t == 0) else black_owned[i]\n",
    "        for i, t in enumerate(turns)\n",
    "    ]\n",
    "    return torch.stack(masks)\n",
    "\n",
    "\n",
    "def hf_batch_iter(ds: Dataset, bs: int):\n",
    "    \"\"\"Iterate over a HF Dataset in shuffled mini‑batches.\"\"\"\n",
    "    ds = ds.shuffle(seed=random.randint(0, 1_000_000))\n",
    "    for start in range(0, len(ds), bs):\n",
    "        yield ds.select(range(start, min(start + bs, len(ds))))\n",
    "\n",
    "\n",
    "def list_batch_iter(data: List[Dict], bs: int):\n",
    "    random.shuffle(data)\n",
    "    for i in range(0, len(data), bs):\n",
    "        yield data[i : i + bs]\n",
    "\n",
    "\n",
    "# =============== Training loop ===============\n",
    "def train(model: nn.Module, dataset):\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    model.train()\n",
    "\n",
    "    # If it's a HF Dataset, ask it to output torch tensors for input/output\n",
    "    if isinstance(dataset, Dataset):\n",
    "        dataset = dataset.with_format(type=\"torch\", columns=[\"input\", \"output\"])\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total, correct, agg_loss = 0, 0, 0.0\n",
    "\n",
    "        batcher = hf_batch_iter(dataset, BATCH_SZ) if isinstance(dataset, Dataset) \\\n",
    "                  else list_batch_iter(dataset, BATCH_SZ)\n",
    "\n",
    "        for batch in batcher:\n",
    "            if isinstance(batch, Dataset):               # Hugging‑Face case\n",
    "                boards  = batch[\"input\"].to(DEVICE)      # already LongTensor [B,64]\n",
    "                outs    = batch[\"output\"].to(DEVICE)     # [B,2]\n",
    "                turns   = batch[\"turn\"]                  # still python list/ndarray\n",
    "            else:                                        # list-of-dicts case\n",
    "                boards  = torch.stack([b[\"input\"] for b in batch]).to(DEVICE)\n",
    "                outs    = torch.stack([b[\"output\"] for b in batch]).to(DEVICE)\n",
    "                turns   = [b[\"turn\"] for b in batch]\n",
    "\n",
    "            targets = (outs[:, 0] * 64 + outs[:, 1]).to(DEVICE)          # [B]\n",
    "            turn_mask = make_turn_mask(boards, turns).to(DEVICE)\n",
    "\n",
    "            logits = model(boards, turn_mask)\n",
    "            loss   = F.cross_entropy(logits, targets)\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            agg_loss += loss.item() * boards.size(0)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            total    += boards.size(0)\n",
    "            correct  += (logits.argmax(dim=-1) == targets).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | loss {agg_loss/total:.4f} | acc {100*correct/total:.2f}%\")\n",
    "\n",
    "\n",
    "# # =============== Main ===============\n",
    "# if __name__ == \"__main__\":\n",
    "#     # ---------------------------------------------------------------------\n",
    "#     # Supply either:\n",
    "#     #   • dataset: List[Dict]    (python list)\n",
    "#     #   • dataset: datasets.Dataset  (already built, with split bishops)\n",
    "#     # ---------------------------------------------------------------------\n",
    "#     dataset = ...  # ← insert your Dataset object or python‑list here\n",
    "#     print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "#     model = ChessMoveModel().to(DEVICE)\n",
    "#     train(model, dataset)\n",
    "\n",
    "#     # torch.save(model.state_dict(), \"chess_split_bishops_transformer.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChessMoveModel().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.8266\n",
      "Loss: 4.4609\n",
      "Loss: 5.1473\n",
      "Loss: 9.8373\n",
      "Loss: 6.2641\n",
      "Loss: 2.4945\n",
      "Loss: 5.1563\n",
      "Loss: 6.4276\n",
      "Loss: 6.4022\n",
      "Loss: 4.1561\n",
      "Loss: 4.5891\n",
      "Loss: 8.7551\n",
      "Loss: 4.5865\n",
      "Loss: 6.0529\n",
      "Loss: 6.2152\n",
      "Loss: 5.9307\n",
      "Loss: 5.3208\n",
      "Loss: 3.3181\n",
      "Loss: 6.2891\n",
      "Loss: 3.1643\n",
      "Loss: 4.8770\n",
      "Loss: 4.2761\n",
      "Loss: 5.2825\n",
      "Loss: 5.6304\n",
      "Loss: 8.7402\n",
      "Loss: 7.2991\n",
      "Loss: 4.4057\n",
      "Loss: 6.7966\n",
      "Loss: 4.2684\n",
      "Loss: 3.0531\n",
      "Loss: 6.1960\n",
      "Loss: 7.8238\n",
      "Loss: 6.4094\n",
      "Loss: 3.8770\n",
      "Loss: 11.6176\n",
      "Loss: 7.6597\n",
      "Loss: 8.8364\n",
      "Loss: 7.1482\n",
      "Loss: 4.4741\n",
      "Loss: 6.1272\n",
      "Loss: 4.9766\n",
      "Loss: 4.1329\n",
      "Loss: 4.5895\n",
      "Loss: 3.1525\n",
      "Loss: 6.6231\n",
      "Loss: 3.8012\n",
      "Loss: 2.5324\n",
      "Loss: 5.7185\n",
      "Loss: 9.1834\n",
      "Loss: 6.0745\n",
      "Loss: 6.8239\n",
      "Loss: 7.1409\n",
      "Loss: 5.4639\n",
      "Loss: 4.4346\n",
      "Loss: 4.2987\n",
      "Loss: 6.6895\n",
      "Loss: 6.4287\n",
      "Loss: 5.2054\n",
      "Loss: 4.3897\n",
      "Loss: 6.3267\n",
      "Loss: 6.6747\n",
      "Loss: 5.1607\n",
      "Loss: 3.9339\n",
      "Loss: 4.5935\n",
      "Loss: 4.6025\n",
      "Loss: 5.0308\n",
      "Loss: 5.2208\n",
      "Loss: 5.7789\n",
      "Loss: 5.6781\n",
      "Loss: 6.0125\n",
      "Loss: 3.5026\n",
      "Loss: 3.8318\n",
      "Loss: 4.5714\n",
      "Loss: 5.3498\n",
      "Loss: 2.8557\n",
      "Loss: 3.5446\n",
      "Loss: 6.3277\n",
      "Loss: 3.5447\n",
      "Loss: 5.0618\n",
      "Loss: 1.8912\n",
      "Loss: 5.2734\n",
      "Loss: 5.3489\n",
      "Loss: 5.7582\n",
      "Loss: 3.8990\n",
      "Loss: 5.5830\n",
      "Loss: 2.0035\n",
      "Loss: 3.9096\n",
      "Loss: 6.0048\n",
      "Loss: 7.2377\n",
      "Loss: 8.5699\n",
      "Loss: 5.1793\n",
      "Loss: 4.4625\n",
      "Loss: 5.2479\n",
      "Loss: 6.9676\n",
      "Loss: 4.2311\n",
      "Loss: 4.5420\n",
      "Loss: 1.9421\n",
      "Loss: 2.9528\n",
      "Loss: 6.8322\n",
      "Loss: 3.7693\n",
      "Loss: 4.3400\n",
      "Loss: 4.7316\n",
      "Loss: 5.6688\n",
      "Loss: 7.1755\n",
      "Loss: 4.7049\n",
      "Loss: 4.5817\n",
      "Loss: 5.1266\n",
      "Loss: 4.9011\n",
      "Loss: 3.6378\n",
      "Loss: 4.1844\n",
      "Loss: 6.5948\n",
      "Loss: 3.9908\n",
      "Loss: 4.9086\n",
      "Loss: 5.3295\n",
      "Loss: 5.2092\n",
      "Loss: 5.5419\n",
      "Loss: 6.7699\n",
      "Loss: 4.9701\n",
      "Loss: 5.5239\n",
      "Loss: 3.7201\n",
      "Loss: 3.3639\n",
      "Loss: 4.4100\n",
      "Loss: 8.5801\n",
      "Loss: 7.1810\n",
      "Loss: 4.6434\n",
      "Loss: 5.3460\n",
      "Loss: 5.0392\n",
      "Loss: 5.0394\n",
      "Loss: 6.3038\n",
      "Loss: 5.8533\n",
      "Loss: 3.9588\n",
      "Loss: 4.4645\n",
      "Loss: 3.9925\n",
      "Loss: 5.3101\n",
      "Loss: 4.1346\n",
      "Loss: 4.4349\n",
      "Loss: 6.5468\n",
      "Loss: 2.8508\n",
      "Loss: 3.3720\n",
      "Loss: 2.8775\n",
      "Loss: 9.8049\n",
      "Loss: 3.1901\n",
      "Loss: 7.2636\n",
      "Loss: 5.0032\n",
      "Loss: 2.6402\n",
      "Loss: 3.0754\n",
      "Loss: 4.1202\n",
      "Loss: 4.8528\n",
      "Loss: 4.7755\n",
      "Loss: 5.3449\n",
      "Loss: 3.7631\n",
      "Loss: 6.7243\n",
      "Loss: 4.3351\n",
      "Loss: 4.2422\n",
      "Loss: 3.2084\n",
      "Loss: 6.5133\n",
      "Loss: 3.6467\n",
      "Loss: 3.7815\n",
      "Loss: 5.9162\n",
      "Loss: 6.3605\n",
      "Loss: 5.8086\n",
      "Loss: 3.6508\n",
      "Loss: 4.8529\n",
      "Loss: 5.0984\n",
      "Loss: 3.3356\n",
      "Loss: 8.9300\n",
      "Loss: 5.0949\n",
      "Loss: 3.4666\n",
      "Loss: 4.0130\n",
      "Loss: 5.1995\n",
      "Loss: 4.5980\n",
      "Loss: 5.8392\n",
      "Loss: 6.2851\n",
      "Loss: 4.8737\n",
      "Loss: 5.4640\n",
      "Loss: 5.3822\n",
      "Loss: 3.7651\n",
      "Loss: 4.6976\n",
      "Loss: 2.6361\n",
      "Loss: 5.5510\n",
      "Loss: 6.4195\n",
      "Loss: 5.3283\n",
      "Loss: 5.5036\n",
      "Loss: 4.8246\n",
      "Loss: 1.5089\n",
      "Loss: 2.6524\n",
      "Loss: 4.6277\n",
      "Loss: 2.1966\n",
      "Loss: 4.8305\n",
      "Loss: 7.3712\n",
      "Loss: 4.4195\n",
      "Loss: 3.1441\n",
      "Loss: 6.0068\n",
      "Loss: 3.3517\n",
      "Loss: 5.3987\n",
      "Loss: 4.9287\n",
      "Loss: 6.0108\n",
      "Loss: 3.4985\n",
      "Loss: 7.1948\n",
      "Loss: 5.6762\n",
      "Loss: 3.8826\n",
      "Loss: 4.2119\n",
      "Loss: 6.8755\n",
      "Loss: 4.0955\n",
      "Loss: 8.0890\n",
      "Loss: 8.0139\n",
      "Loss: 2.7119\n",
      "Loss: 7.5031\n",
      "Loss: 5.2589\n",
      "Loss: 5.8849\n",
      "Loss: 5.4280\n",
      "Loss: 4.5325\n",
      "Loss: 5.9171\n",
      "Loss: 5.2073\n",
      "Loss: 5.3687\n",
      "Loss: 8.0604\n",
      "Loss: 5.2621\n",
      "Loss: 4.2121\n",
      "Loss: 4.4461\n",
      "Loss: 5.1927\n",
      "Loss: 4.6261\n",
      "Loss: 4.1467\n",
      "Loss: 5.5207\n",
      "Loss: 3.1359\n",
      "Loss: 4.0114\n",
      "Loss: 4.6527\n",
      "Loss: 4.0883\n",
      "Loss: 7.3776\n",
      "Loss: 3.8759\n",
      "Loss: 4.0600\n",
      "Loss: 4.5225\n",
      "Loss: 7.9640\n",
      "Loss: 3.4489\n",
      "Loss: 4.1871\n",
      "Loss: 3.3320\n",
      "Loss: 4.5157\n",
      "Loss: 3.2186\n",
      "Loss: 7.2471\n",
      "Loss: 5.3761\n",
      "Loss: 4.6249\n",
      "Loss: 5.8676\n",
      "Loss: 7.5408\n",
      "Loss: 3.3555\n",
      "Loss: 2.1731\n",
      "Loss: 3.7124\n",
      "Loss: 9.1316\n",
      "Loss: 4.2039\n",
      "Loss: 5.0640\n",
      "Loss: 6.8425\n",
      "Loss: 4.8358\n",
      "Loss: 6.3252\n",
      "Loss: 6.1480\n",
      "Loss: 2.9804\n",
      "Loss: 3.2477\n",
      "Loss: 4.8569\n",
      "Loss: 5.6015\n",
      "Loss: 4.5684\n",
      "Loss: 4.9186\n",
      "Loss: 3.6853\n",
      "Loss: 4.4163\n",
      "Loss: 7.5673\n",
      "Loss: 5.1776\n",
      "Loss: 6.7121\n",
      "Loss: 5.3456\n",
      "Loss: 3.4516\n",
      "Loss: 2.4188\n",
      "Loss: 4.1283\n",
      "Loss: 3.4932\n",
      "Loss: 3.7694\n",
      "Loss: 3.2312\n",
      "Loss: 4.8597\n",
      "Loss: 5.5596\n",
      "Loss: 4.9372\n",
      "Loss: 5.5471\n",
      "Loss: 4.9432\n",
      "Loss: 4.1705\n",
      "Loss: 6.1184\n",
      "Loss: 4.8102\n",
      "Loss: 3.9174\n",
      "Loss: 3.3566\n",
      "Loss: 3.7964\n",
      "Loss: 4.6722\n",
      "Loss: 3.6500\n",
      "Loss: 7.0689\n",
      "Loss: 5.0960\n",
      "Loss: 9.0673\n",
      "Loss: 7.9580\n",
      "Loss: 5.7421\n",
      "Loss: 5.2918\n",
      "Loss: 5.8819\n",
      "Loss: 5.2837\n",
      "Loss: 5.4170\n",
      "Loss: 4.2809\n",
      "Loss: 2.6249\n",
      "Loss: 8.3175\n",
      "Loss: 5.2626\n",
      "Loss: 3.8261\n",
      "Loss: 6.5248\n",
      "Loss: 4.8334\n",
      "Loss: 6.1481\n",
      "Loss: 4.9021\n",
      "Loss: 4.2740\n",
      "Loss: 4.3569\n",
      "Loss: 4.1493\n",
      "Loss: 3.1910\n",
      "Loss: 2.0746\n",
      "Loss: 3.9038\n",
      "Loss: 5.5059\n",
      "Loss: 3.6928\n",
      "Loss: 6.5219\n",
      "Loss: 5.8827\n",
      "Loss: 1.7848\n",
      "Loss: 5.6561\n",
      "Loss: 6.6949\n",
      "Loss: 10.1839\n",
      "Loss: 3.3112\n",
      "Loss: 3.5702\n",
      "Loss: 5.4531\n",
      "Loss: 5.4454\n",
      "Loss: 3.5417\n",
      "Loss: 5.2784\n",
      "Loss: 3.9924\n",
      "Loss: 6.5593\n",
      "Loss: 7.2353\n",
      "Loss: 4.7980\n",
      "Loss: 4.6847\n",
      "Loss: 2.6440\n",
      "Loss: 4.7931\n",
      "Loss: 9.4823\n",
      "Loss: 6.0832\n",
      "Loss: 3.8634\n",
      "Loss: 4.5817\n",
      "Loss: 2.2674\n",
      "Loss: 6.2907\n",
      "Loss: 7.0417\n",
      "Loss: 3.9305\n",
      "Loss: 8.5908\n",
      "Loss: 6.0289\n",
      "Loss: 3.1688\n",
      "Loss: 2.6547\n",
      "Loss: 4.7382\n",
      "Loss: 6.4050\n",
      "Loss: 6.3255\n",
      "Loss: 3.8772\n",
      "Loss: 7.3456\n",
      "Loss: 7.1186\n",
      "Loss: 4.5195\n",
      "Loss: 5.0770\n",
      "Loss: 9.5047\n",
      "Loss: 3.3784\n",
      "Loss: 8.2465\n",
      "Loss: 5.3000\n",
      "Loss: 5.1265\n",
      "Loss: 4.5524\n",
      "Loss: 8.2589\n",
      "Loss: 4.6512\n",
      "Loss: 5.1769\n",
      "Loss: 8.3954\n",
      "Loss: 4.0408\n",
      "Loss: 2.3103\n",
      "Loss: 3.5209\n",
      "Loss: 3.5015\n",
      "Loss: 3.0416\n",
      "Loss: 6.1324\n",
      "Loss: 4.7271\n",
      "Loss: 6.3438\n",
      "Loss: 4.0536\n",
      "Loss: 6.2362\n",
      "Loss: 6.6795\n",
      "Loss: 3.8450\n",
      "Loss: 5.6252\n",
      "Loss: 6.4588\n",
      "Loss: 5.5207\n",
      "Loss: 7.2265\n",
      "Loss: 6.4594\n",
      "Loss: 7.1108\n",
      "Loss: 5.6506\n",
      "Loss: 6.1595\n",
      "Loss: 4.6415\n",
      "Loss: 3.9941\n",
      "Loss: 3.9089\n",
      "Loss: 5.4849\n",
      "Loss: 5.2786\n",
      "Loss: 6.0093\n",
      "Loss: 4.9949\n",
      "Loss: 5.2976\n",
      "Loss: 5.1682\n",
      "Loss: 4.0795\n",
      "Loss: 5.7018\n",
      "Loss: 3.4287\n",
      "Loss: 3.8019\n",
      "Loss: 2.9702\n",
      "Loss: 4.6034\n",
      "Loss: 3.3728\n",
      "Loss: 5.4738\n",
      "Loss: 4.7725\n",
      "Loss: 4.3949\n",
      "Loss: 5.7850\n",
      "Loss: 4.2076\n",
      "Loss: 5.0778\n",
      "Loss: 6.0276\n",
      "Loss: 4.0815\n",
      "Loss: 5.0487\n",
      "Loss: 4.8280\n",
      "Loss: 4.8869\n",
      "Loss: 5.1129\n",
      "Loss: 6.4997\n",
      "Loss: 3.1390\n",
      "Loss: 6.0637\n",
      "Loss: 3.8140\n",
      "Loss: 7.1982\n",
      "Loss: 5.4700\n",
      "Loss: 3.8635\n",
      "Loss: 5.3370\n",
      "Loss: 7.9410\n",
      "Loss: 4.1130\n",
      "Loss: 5.2208\n",
      "Loss: 7.4868\n",
      "Loss: 5.7306\n",
      "Loss: 3.7785\n",
      "Loss: 3.3717\n",
      "Loss: 3.3111\n",
      "Loss: 4.3222\n",
      "Loss: 3.8888\n",
      "Loss: 4.8431\n",
      "Loss: 4.9554\n",
      "Loss: 7.3465\n",
      "Loss: 4.6936\n",
      "Loss: 6.8685\n",
      "Loss: 3.5276\n",
      "Loss: 4.7730\n",
      "Loss: 4.2064\n",
      "Loss: 4.4393\n",
      "Loss: 5.9489\n",
      "Loss: 4.8814\n",
      "Loss: 7.3066\n",
      "Loss: 5.0072\n",
      "Loss: 5.0669\n",
      "Loss: 4.2747\n",
      "Loss: 4.3148\n",
      "Loss: 3.9206\n",
      "Loss: 9.9261\n",
      "Loss: 5.2839\n",
      "Loss: 5.9634\n",
      "Loss: 5.9235\n",
      "Loss: 4.9097\n",
      "Loss: 2.5314\n",
      "Loss: 4.9206\n",
      "Loss: 7.0495\n",
      "Loss: 4.2200\n",
      "Loss: 2.9745\n",
      "Loss: 7.2759\n",
      "Loss: 3.5554\n",
      "Loss: 4.1848\n",
      "Loss: 8.8221\n",
      "Loss: 7.9146\n",
      "Loss: 6.0756\n",
      "Loss: 2.0377\n",
      "Loss: 4.2323\n",
      "Loss: 4.3681\n",
      "Loss: 4.8390\n",
      "Loss: 3.6226\n",
      "Loss: 4.9968\n",
      "Loss: 3.5162\n",
      "Loss: 4.4488\n",
      "Loss: 4.6332\n",
      "Loss: 3.6751\n",
      "Loss: 5.4500\n",
      "Loss: 6.5955\n",
      "Loss: 7.7164\n",
      "Loss: 4.1811\n",
      "Loss: 5.2750\n",
      "Loss: 4.4947\n",
      "Loss: 5.0012\n",
      "Loss: 3.9192\n",
      "Loss: 3.8590\n",
      "Loss: 6.2310\n",
      "Loss: 6.8087\n",
      "Loss: 2.9287\n",
      "Loss: 3.7450\n",
      "Loss: 6.3330\n",
      "Loss: 6.8717\n",
      "Loss: 6.5992\n",
      "Loss: 4.4335\n",
      "Loss: 4.7331\n",
      "Loss: 4.5068\n",
      "Loss: 3.2912\n",
      "Loss: 4.2460\n",
      "Loss: 4.4829\n",
      "Loss: 3.9313\n",
      "Loss: 5.9427\n",
      "Loss: 5.8439\n",
      "Loss: 4.4898\n",
      "Loss: 2.4814\n",
      "Loss: 3.3863\n",
      "Loss: 4.6625\n",
      "Loss: 6.3088\n",
      "Loss: 3.0591\n",
      "Loss: 6.6018\n",
      "Loss: 3.1739\n",
      "Loss: 2.7669\n",
      "Loss: 4.1954\n",
      "Loss: 7.8525\n",
      "Loss: 3.6438\n",
      "Loss: 1.8180\n",
      "Loss: 4.3851\n",
      "Loss: 5.5944\n",
      "Loss: 2.9319\n",
      "Loss: 3.8325\n",
      "Loss: 4.6282\n",
      "Loss: 3.6099\n",
      "Loss: 4.5593\n",
      "Loss: 7.9560\n",
      "Loss: 2.9221\n",
      "Loss: 6.0689\n",
      "Loss: 3.6459\n",
      "Loss: 3.6499\n",
      "Loss: 4.7798\n",
      "Loss: 6.4004\n",
      "Loss: 2.8961\n",
      "Loss: 4.1823\n",
      "Loss: 4.3036\n",
      "Loss: 2.8785\n",
      "Loss: 6.2161\n",
      "Loss: 4.9501\n",
      "Loss: 5.5186\n",
      "Loss: 5.2444\n",
      "Loss: 6.1917\n",
      "Loss: 2.2585\n",
      "Loss: 3.8147\n",
      "Loss: 4.8496\n",
      "Loss: 6.9190\n",
      "Loss: 4.5076\n",
      "Loss: 5.0967\n",
      "Loss: 4.2627\n",
      "Loss: 5.3575\n",
      "Loss: 5.7980\n",
      "Loss: 5.6658\n",
      "Loss: 4.4879\n",
      "Loss: 4.7711\n",
      "Loss: 4.1232\n",
      "Loss: 5.2173\n",
      "Loss: 5.0205\n",
      "Loss: 4.7202\n",
      "Loss: 3.6300\n",
      "Loss: 4.7326\n",
      "Loss: 3.7918\n",
      "Loss: 5.6842\n",
      "Loss: 3.7763\n",
      "Loss: 4.1696\n",
      "Loss: 3.5076\n",
      "Loss: 5.7728\n",
      "Loss: 4.2460\n",
      "Loss: 7.0117\n",
      "Loss: 4.8641\n",
      "Loss: 7.3020\n",
      "Loss: 5.5926\n",
      "Loss: 5.6850\n",
      "Loss: 3.1977\n",
      "Loss: 5.3099\n",
      "Loss: 5.1078\n",
      "Loss: 3.7769\n",
      "Loss: 4.4407\n",
      "Loss: 3.6001\n",
      "Loss: 3.3398\n",
      "Loss: 3.3347\n",
      "Loss: 3.2783\n",
      "Loss: 5.7619\n",
      "Loss: 4.7351\n",
      "Loss: 3.7707\n",
      "Loss: 4.4545\n",
      "Loss: 4.6325\n",
      "Loss: 4.2826\n",
      "Loss: 2.1912\n",
      "Loss: 3.6377\n",
      "Loss: 4.3260\n",
      "Loss: 5.6630\n",
      "Loss: 7.6361\n",
      "Loss: 4.9225\n",
      "Loss: 4.8196\n",
      "Loss: 5.8039\n",
      "Loss: 4.9889\n",
      "Loss: 5.0365\n",
      "Loss: 4.6416\n",
      "Loss: 5.5370\n",
      "Loss: 2.9968\n",
      "Loss: 5.4223\n",
      "Loss: 7.8923\n",
      "Loss: 5.1078\n",
      "Loss: 4.3978\n",
      "Loss: 2.4431\n",
      "Loss: 3.5260\n",
      "Loss: 2.3867\n",
      "Loss: 4.4423\n",
      "Loss: 4.3285\n",
      "Loss: 3.1828\n",
      "Loss: 6.0374\n",
      "Loss: 3.0349\n",
      "Loss: 7.5952\n",
      "Loss: 5.0491\n",
      "Loss: 5.5508\n",
      "Loss: 6.4399\n",
      "Loss: 4.7959\n",
      "Loss: 4.4254\n",
      "Loss: 4.9277\n",
      "Loss: 4.1861\n",
      "Loss: 4.9193\n",
      "Loss: 5.0753\n",
      "Loss: 7.7488\n",
      "Loss: 4.4212\n",
      "Loss: 3.8193\n",
      "Loss: 5.3318\n",
      "Loss: 4.5707\n",
      "Loss: 7.7114\n",
      "Loss: 7.2619\n",
      "Loss: 4.8355\n",
      "Loss: 5.2437\n",
      "Loss: 5.6597\n",
      "Loss: 3.3829\n",
      "Loss: 3.3628\n",
      "Loss: 7.8392\n",
      "Loss: 3.7330\n",
      "Loss: 1.6691\n",
      "Loss: 6.1311\n",
      "Loss: 5.6514\n",
      "Loss: 4.1817\n",
      "Loss: 4.4647\n",
      "Loss: 5.2602\n",
      "Loss: 3.6000\n",
      "Loss: 4.9752\n",
      "Loss: 3.9969\n",
      "Loss: 7.9560\n",
      "Loss: 7.0444\n",
      "Loss: 6.0046\n",
      "Loss: 7.2596\n",
      "Loss: 4.4426\n",
      "Loss: 4.9993\n",
      "Loss: 6.7693\n",
      "Loss: 5.8680\n",
      "Loss: 3.5028\n",
      "Loss: 2.3382\n",
      "Loss: 4.3890\n",
      "Loss: 3.8666\n",
      "Loss: 7.2722\n",
      "Loss: 4.7920\n",
      "Loss: 8.4049\n",
      "Loss: 3.5973\n",
      "Loss: 2.9897\n",
      "Loss: 3.1943\n",
      "Loss: 5.1067\n",
      "Loss: 6.2041\n",
      "Loss: 4.2680\n",
      "Loss: 8.4008\n",
      "Loss: 3.6352\n",
      "Loss: 5.5368\n",
      "Loss: 2.4458\n",
      "Loss: 6.3120\n",
      "Loss: 3.8687\n",
      "Loss: 3.2097\n",
      "Loss: 3.8108\n",
      "Loss: 5.4247\n",
      "Loss: 4.3854\n",
      "Loss: 6.5218\n",
      "Loss: 4.8752\n",
      "Loss: 5.8243\n",
      "Loss: 3.3843\n",
      "Loss: 4.0618\n",
      "Loss: 4.2400\n",
      "Loss: 3.9708\n",
      "Loss: 7.9418\n",
      "Loss: 6.0842\n",
      "Loss: 5.0484\n",
      "Loss: 3.7485\n",
      "Loss: 8.8860\n",
      "Loss: 5.8105\n",
      "Loss: 4.8920\n",
      "Loss: 6.3737\n",
      "Loss: 4.2273\n",
      "Loss: 8.2988\n",
      "Loss: 4.1436\n",
      "Loss: 2.9632\n",
      "Loss: 3.5227\n",
      "Loss: 4.5475\n",
      "Loss: 4.3307\n",
      "Loss: 6.2621\n",
      "Loss: 4.2460\n",
      "Loss: 4.7680\n",
      "Loss: 5.3136\n",
      "Loss: 6.1476\n",
      "Loss: 5.5821\n",
      "Loss: 7.3517\n",
      "Loss: 3.1810\n",
      "Loss: 4.6105\n",
      "Loss: 6.4994\n",
      "Loss: 3.9360\n",
      "Loss: 2.5543\n",
      "Loss: 5.1042\n",
      "Loss: 5.9255\n",
      "Loss: 5.4297\n",
      "Loss: 4.4039\n",
      "Loss: 5.7180\n",
      "Loss: 4.5109\n",
      "Loss: 8.9215\n",
      "Loss: 2.4261\n",
      "Loss: 6.9475\n",
      "Loss: 5.3780\n",
      "Loss: 6.1833\n",
      "Loss: 4.3817\n",
      "Loss: 4.4458\n",
      "Loss: 7.2529\n",
      "Loss: 2.8877\n",
      "Loss: 4.2682\n",
      "Loss: 4.4191\n",
      "Loss: 4.1171\n",
      "Loss: 6.4205\n",
      "Loss: 3.6112\n",
      "Loss: 4.6434\n",
      "Loss: 3.0829\n",
      "Loss: 4.3918\n",
      "Loss: 5.4182\n",
      "Loss: 4.2075\n",
      "Loss: 3.0563\n",
      "Loss: 5.6946\n",
      "Loss: 5.2296\n",
      "Loss: 4.7247\n",
      "Loss: 6.2957\n",
      "Loss: 5.9150\n",
      "Loss: 3.8176\n",
      "Loss: 4.0335\n",
      "Loss: 4.8445\n",
      "Loss: 4.2521\n",
      "Loss: 5.3810\n",
      "Loss: 3.6268\n",
      "Loss: 8.0647\n",
      "Loss: 4.1750\n",
      "Loss: 4.8456\n",
      "Loss: 3.4661\n",
      "Loss: 3.8508\n",
      "Loss: 4.7421\n",
      "Loss: 2.9508\n",
      "Loss: 4.8540\n",
      "Loss: 4.1101\n",
      "Loss: 4.6582\n",
      "Loss: 4.9426\n",
      "Loss: 6.4115\n",
      "Loss: 4.7758\n",
      "Loss: 5.9089\n",
      "Loss: 4.2202\n",
      "Loss: 4.3619\n",
      "Loss: 5.6876\n",
      "Loss: 5.7204\n",
      "Loss: 4.8280\n",
      "Loss: 3.3977\n",
      "Loss: 8.0880\n",
      "Loss: 4.2872\n",
      "Loss: 4.8361\n",
      "Loss: 8.3575\n",
      "Loss: 5.7973\n",
      "Loss: 3.1838\n",
      "Loss: 4.9642\n",
      "Loss: 7.4137\n",
      "Loss: 7.9680\n",
      "Loss: 4.9383\n",
      "Loss: 4.4689\n",
      "Loss: 5.4453\n",
      "Loss: 3.2146\n",
      "Loss: 6.7069\n",
      "Loss: 4.0557\n",
      "Loss: 4.2597\n",
      "Loss: 3.8267\n",
      "Loss: 2.8708\n",
      "Loss: 3.2897\n",
      "Loss: 4.6012\n",
      "Loss: 5.6891\n",
      "Loss: 5.3849\n",
      "Loss: 5.4301\n",
      "Loss: 3.4322\n",
      "Loss: 6.6330\n",
      "Loss: 4.9473\n",
      "Loss: 6.3864\n",
      "Loss: 7.9380\n",
      "Loss: 3.7139\n",
      "Loss: 3.5428\n",
      "Loss: 4.5053\n",
      "Loss: 3.8666\n",
      "Loss: 7.9715\n",
      "Loss: 2.4520\n",
      "Loss: 3.8323\n",
      "Loss: 7.0807\n",
      "Loss: 4.7913\n",
      "Loss: 5.2797\n",
      "Loss: 6.7249\n",
      "Loss: 6.5508\n",
      "Loss: 7.0639\n",
      "Loss: 6.2874\n",
      "Loss: 6.6391\n",
      "Loss: 6.2625\n",
      "Loss: 5.1130\n",
      "Loss: 5.3577\n",
      "Loss: 4.3371\n",
      "Loss: 5.3467\n",
      "Loss: 6.1575\n",
      "Loss: 5.2174\n",
      "Loss: 3.5476\n",
      "Loss: 5.2757\n",
      "Loss: 9.5028\n",
      "Loss: 5.3357\n",
      "Loss: 5.6479\n",
      "Loss: 3.5407\n",
      "Loss: 7.5016\n",
      "Loss: 4.5429\n",
      "Loss: 4.6089\n",
      "Loss: 5.1920\n",
      "Loss: 7.0624\n",
      "Loss: 4.0880\n",
      "Loss: 4.4972\n",
      "Loss: 3.6463\n",
      "Loss: 7.6964\n",
      "Loss: 2.2376\n",
      "Loss: 3.6504\n",
      "Loss: 4.4081\n",
      "Loss: 6.0107\n",
      "Loss: 4.9144\n",
      "Loss: 5.0171\n",
      "Loss: 4.2254\n",
      "Loss: 4.8556\n",
      "Loss: 3.9803\n",
      "Loss: 4.7713\n",
      "Loss: 3.1292\n",
      "Loss: 5.1559\n",
      "Loss: 7.0101\n",
      "Loss: 5.4969\n",
      "Loss: 7.0829\n",
      "Loss: 6.8548\n",
      "Loss: 4.5660\n",
      "Loss: 5.7175\n",
      "Loss: 4.7622\n",
      "Loss: 5.6826\n",
      "Loss: 4.6471\n",
      "Loss: 3.8042\n",
      "Loss: 6.9272\n",
      "Loss: 4.8161\n",
      "Loss: 4.7825\n",
      "Loss: 5.2139\n",
      "Loss: 6.1183\n",
      "Loss: 2.4959\n",
      "Loss: 3.7493\n",
      "Loss: 5.3348\n",
      "Loss: 4.9893\n",
      "Loss: 3.7450\n",
      "Loss: 4.6374\n",
      "Loss: 7.2422\n",
      "Loss: 3.8908\n",
      "Loss: 4.6685\n",
      "Loss: 4.8673\n",
      "Loss: 10.1081\n",
      "Loss: 6.7375\n",
      "Loss: 5.3261\n",
      "Loss: 5.1992\n",
      "Loss: 4.3997\n",
      "Loss: 3.5358\n",
      "Loss: 4.8040\n",
      "Loss: 2.6435\n",
      "Loss: 6.7611\n",
      "Loss: 3.5425\n",
      "Loss: 8.2124\n",
      "Loss: 5.3683\n",
      "Loss: 4.6143\n",
      "Loss: 4.6547\n",
      "Loss: 5.8147\n",
      "Loss: 7.8045\n",
      "Loss: 4.1188\n",
      "Loss: 6.2476\n",
      "Loss: 4.5407\n",
      "Loss: 5.9448\n",
      "Loss: 5.0959\n",
      "Loss: 4.6597\n",
      "Loss: 6.8738\n",
      "Loss: 3.9749\n",
      "Loss: 7.9227\n",
      "Loss: 3.2993\n",
      "Loss: 5.0275\n",
      "Loss: 5.1339\n",
      "Loss: 5.4350\n",
      "Loss: 6.7652\n",
      "Loss: 3.3354\n",
      "Loss: 5.4720\n",
      "Loss: 5.9843\n",
      "Loss: 4.2244\n",
      "Loss: 7.6179\n",
      "Loss: 4.8560\n",
      "Loss: 3.8173\n",
      "Loss: 7.4873\n",
      "Loss: 5.6789\n",
      "Loss: 6.8158\n",
      "Loss: 4.5224\n",
      "Loss: 4.2665\n",
      "Loss: 4.2347\n",
      "Loss: 6.1092\n",
      "Loss: 3.8775\n",
      "Loss: 4.4518\n",
      "Loss: 6.0129\n",
      "Loss: 4.4264\n",
      "Loss: 3.6209\n",
      "Loss: 7.5471\n",
      "Loss: 4.6143\n",
      "Loss: 4.3015\n",
      "Loss: 3.8764\n",
      "Loss: 4.6002\n",
      "Loss: 4.4839\n",
      "Loss: 3.0053\n",
      "Loss: 5.8496\n",
      "Loss: 3.3181\n",
      "Loss: 4.1029\n",
      "Loss: 4.3280\n",
      "Loss: 5.7857\n",
      "Loss: 5.6163\n",
      "Loss: 5.3602\n",
      "Loss: 4.7655\n",
      "Loss: 3.1330\n",
      "Loss: 3.6740\n",
      "Loss: 4.5559\n",
      "Loss: 4.1992\n",
      "Loss: 5.5304\n",
      "Loss: 5.1527\n",
      "Loss: 3.5026\n",
      "Loss: 4.3091\n",
      "Loss: 3.5888\n",
      "Loss: 6.5303\n",
      "Loss: 6.3617\n",
      "Loss: 4.6297\n",
      "Loss: 8.2243\n",
      "Loss: 4.7528\n",
      "Loss: 6.6776\n",
      "Loss: 6.5887\n",
      "Loss: 7.0885\n",
      "Loss: 3.4142\n",
      "Loss: 2.8571\n",
      "Loss: 4.2964\n",
      "Loss: 4.0389\n",
      "Loss: 4.1474\n",
      "Loss: 6.7611\n",
      "Loss: 6.1841\n",
      "Loss: 4.4300\n",
      "Loss: 8.3944\n",
      "Loss: 4.6537\n",
      "Loss: 4.6522\n",
      "Loss: 9.0106\n",
      "Loss: 6.2740\n",
      "Loss: 2.8739\n",
      "Loss: 9.9146\n",
      "Loss: 2.3658\n",
      "Loss: 5.6057\n",
      "Loss: 5.1242\n",
      "Loss: 3.2311\n",
      "Loss: 4.9684\n",
      "Loss: 4.2412\n",
      "Loss: 2.6441\n",
      "Loss: 3.9699\n",
      "Loss: 2.3594\n",
      "Loss: 6.4362\n",
      "Loss: 4.0713\n",
      "Loss: 3.8299\n",
      "Loss: 6.5660\n",
      "Loss: 3.6417\n",
      "Loss: 3.0608\n",
      "Loss: 5.1579\n",
      "Loss: 5.2044\n",
      "Loss: 3.8259\n",
      "Loss: 4.7825\n",
      "Loss: 3.7898\n",
      "Loss: 6.0105\n",
      "Loss: 5.8149\n",
      "Loss: 4.8951\n",
      "Loss: 4.8942\n",
      "Loss: 3.6039\n",
      "Loss: 3.5319\n",
      "Loss: 2.9887\n",
      "Loss: 8.4276\n",
      "Loss: 4.5388\n",
      "Loss: 6.1443\n",
      "Loss: 7.0258\n",
      "Loss: 4.6974\n",
      "Loss: 3.1834\n",
      "Loss: 5.3885\n",
      "Loss: 5.4349\n",
      "Loss: 7.1774\n",
      "Loss: 3.6772\n",
      "Loss: 3.1599\n",
      "Loss: 5.0679\n",
      "Loss: 4.4504\n",
      "Loss: 4.7563\n",
      "Loss: 6.1846\n",
      "Loss: 3.4151\n",
      "Loss: 5.5658\n",
      "Loss: 6.8868\n",
      "Loss: 5.3069\n",
      "Loss: 1.8100\n",
      "Loss: 5.4106\n",
      "Loss: 5.1352\n",
      "Loss: 6.2885\n",
      "Loss: 4.4663\n",
      "Loss: 3.5540\n",
      "Loss: 6.5308\n",
      "Loss: 5.2517\n",
      "Loss: 5.0006\n",
      "Loss: 6.5460\n",
      "Loss: 4.5025\n",
      "Loss: 6.0562\n",
      "Loss: 2.8963\n",
      "Loss: 2.9323\n",
      "Loss: 5.3342\n",
      "Loss: 5.1227\n",
      "Loss: 6.2209\n",
      "Loss: 4.8435\n",
      "Loss: 4.2344\n",
      "Loss: 4.9814\n",
      "Loss: 5.4803\n",
      "Loss: 4.5836\n",
      "Loss: 3.3963\n",
      "Loss: 8.0297\n",
      "Loss: 3.6302\n",
      "Loss: 4.7917\n",
      "Loss: 3.2375\n",
      "Loss: 4.7786\n",
      "Loss: 4.5134\n",
      "Loss: 4.9345\n",
      "Loss: 3.2448\n",
      "Loss: 4.5702\n",
      "Loss: 4.5695\n",
      "Loss: 5.0198\n",
      "Loss: 5.8968\n",
      "Loss: 3.1631\n",
      "Loss: 2.8434\n",
      "Loss: 4.3276\n",
      "Loss: 4.3603\n",
      "Loss: 4.3103\n",
      "Loss: 8.4378\n",
      "Loss: 7.0395\n",
      "Loss: 5.1681\n",
      "Loss: 3.7966\n",
      "Loss: 7.9139\n",
      "Loss: 4.4188\n",
      "Loss: 3.0105\n",
      "Loss: 6.8926\n",
      "Loss: 5.2339\n",
      "Loss: 7.6942\n",
      "Loss: 3.7009\n",
      "Loss: 3.2203\n",
      "Loss: 6.3567\n",
      "Loss: 5.1287\n",
      "Loss: 4.6078\n",
      "Loss: 4.5284\n",
      "Loss: 5.0620\n",
      "Loss: 6.0429\n",
      "Loss: 3.7733\n",
      "Loss: 4.8371\n",
      "Loss: 7.6282\n",
      "Loss: 4.9158\n",
      "Loss: 4.4844\n",
      "Loss: 5.5564\n",
      "Loss: 4.7378\n",
      "Loss: 4.9934\n",
      "Loss: 9.3435\n",
      "Loss: 4.4900\n",
      "Loss: 2.3098\n",
      "Loss: 3.6074\n",
      "Loss: 2.8540\n",
      "Loss: 5.9997\n",
      "Loss: 5.6925\n",
      "Loss: 6.8338\n",
      "Loss: 4.8482\n",
      "Loss: 3.7469\n",
      "Loss: 3.2345\n",
      "Loss: 4.3246\n",
      "Loss: 4.0163\n",
      "Loss: 7.2343\n",
      "Loss: 3.6152\n",
      "Loss: 6.4894\n",
      "Loss: 5.3631\n",
      "Loss: 3.1225\n",
      "Loss: 4.5288\n",
      "Loss: 4.6083\n",
      "Loss: 5.0117\n",
      "Loss: 5.5607\n",
      "Loss: 3.6661\n",
      "Loss: 2.3038\n",
      "Loss: 7.3940\n",
      "Loss: 3.8803\n",
      "Loss: 4.8302\n",
      "Loss: 3.7611\n",
      "Loss: 4.8564\n",
      "Loss: 4.1154\n",
      "Loss: 3.9888\n",
      "Loss: 5.3758\n",
      "Loss: 6.7673\n",
      "Loss: 3.9853\n",
      "Loss: 4.4462\n",
      "Loss: 4.9012\n",
      "Loss: 3.8747\n",
      "Loss: 7.8276\n",
      "Loss: 4.4640\n",
      "Loss: 4.4595\n",
      "Loss: 4.2868\n",
      "Loss: 4.3140\n",
      "Loss: 3.5055\n",
      "Loss: 5.3446\n",
      "Loss: 4.2357\n",
      "Loss: 4.7437\n",
      "Loss: 4.0845\n",
      "Loss: 4.4805\n",
      "Loss: 5.0910\n",
      "Loss: 4.9103\n",
      "Loss: 8.2298\n",
      "Loss: 8.5972\n",
      "Loss: 6.2269\n",
      "Loss: 5.7043\n",
      "Loss: 4.9161\n",
      "Loss: 6.0241\n",
      "Loss: 6.5941\n",
      "Loss: 4.5231\n",
      "Loss: 5.3866\n",
      "Loss: 4.8086\n",
      "Loss: 3.8841\n",
      "Loss: 4.5597\n",
      "Loss: 5.7645\n",
      "Loss: 5.0359\n",
      "Loss: 5.2310\n",
      "Loss: 5.1920\n",
      "Loss: 3.4568\n",
      "Loss: 4.9758\n",
      "Loss: 5.9401\n",
      "Loss: 6.8490\n",
      "Loss: 5.0103\n",
      "Loss: 4.9207\n",
      "Loss: 4.5464\n",
      "Loss: 7.0488\n",
      "Loss: 6.1861\n",
      "Loss: 4.2379\n",
      "Loss: 5.1397\n",
      "Loss: 5.0719\n",
      "Loss: 6.0368\n",
      "Loss: 5.6432\n",
      "Loss: 7.6894\n",
      "Loss: 4.0337\n",
      "Loss: 3.1282\n",
      "Loss: 4.7484\n",
      "Loss: 4.9299\n",
      "Loss: 6.4685\n",
      "Loss: 3.1086\n",
      "Loss: 6.3026\n",
      "Loss: 4.7625\n",
      "Loss: 5.8614\n",
      "Loss: 3.8270\n",
      "Loss: 5.0560\n",
      "Loss: 2.5475\n",
      "Loss: 8.2989\n",
      "Loss: 6.2013\n",
      "Loss: 5.5652\n",
      "Loss: 4.4905\n",
      "Loss: 5.4859\n",
      "Loss: 5.1086\n",
      "Loss: 8.6584\n",
      "Loss: 4.3403\n",
      "Loss: 8.4833\n",
      "Loss: 4.6730\n",
      "Loss: 2.7450\n",
      "Loss: 3.4419\n",
      "Loss: 6.0117\n",
      "Loss: 5.2322\n",
      "Loss: 4.3809\n",
      "Loss: 3.4786\n",
      "Loss: 3.5128\n",
      "Loss: 6.1437\n",
      "Loss: 4.5741\n",
      "Loss: 5.3539\n",
      "Loss: 3.4713\n",
      "Loss: 5.5739\n",
      "Loss: 5.0112\n",
      "Loss: 4.1387\n",
      "Loss: 3.7733\n",
      "Loss: 3.6992\n",
      "Loss: 4.6791\n",
      "Loss: 8.5839\n",
      "Loss: 2.9255\n",
      "Loss: 6.3211\n",
      "Loss: 3.0121\n",
      "Loss: 7.0245\n",
      "Loss: 3.2235\n",
      "Loss: 5.0513\n",
      "Loss: 5.7305\n",
      "Loss: 5.7446\n",
      "Loss: 4.1099\n",
      "Loss: 4.0881\n",
      "Loss: 5.7893\n",
      "Loss: 3.5051\n",
      "Loss: 4.6150\n",
      "Loss: 5.0905\n",
      "Loss: 3.1413\n",
      "Loss: 3.5370\n",
      "Loss: 6.6275\n",
      "Loss: 4.5547\n",
      "Loss: 3.9439\n",
      "Loss: 4.7487\n",
      "Loss: 4.4713\n",
      "Loss: 5.3155\n",
      "Loss: 4.5308\n",
      "Loss: 5.0607\n",
      "Loss: 5.6256\n",
      "Loss: 5.8289\n",
      "Loss: 4.8518\n",
      "Loss: 6.1111\n",
      "Loss: 3.7316\n",
      "Loss: 2.4390\n",
      "Loss: 5.0310\n",
      "Loss: 3.8507\n",
      "Loss: 3.4647\n",
      "Loss: 4.2663\n",
      "Loss: 2.5852\n",
      "Loss: 5.6085\n",
      "Loss: 4.3155\n",
      "Loss: 3.8049\n",
      "Loss: 5.0635\n",
      "Loss: 9.5888\n",
      "Loss: 5.3129\n",
      "Loss: 5.4308\n",
      "Loss: 6.0960\n",
      "Loss: 4.9630\n",
      "Loss: 4.3823\n",
      "Loss: 5.0839\n",
      "Loss: 6.8132\n",
      "Loss: 4.0610\n",
      "Loss: 4.5738\n",
      "Loss: 8.1137\n",
      "Loss: 3.5001\n",
      "Loss: 5.7605\n",
      "Loss: 4.8174\n",
      "Loss: 4.6836\n",
      "Loss: 5.4699\n",
      "Loss: 6.9550\n",
      "Loss: 5.4719\n",
      "Loss: 3.5115\n",
      "Loss: 4.5193\n",
      "Loss: 7.5415\n",
      "Loss: 4.2940\n",
      "Loss: 8.7955\n",
      "Loss: 3.4078\n",
      "Loss: 5.2588\n",
      "Loss: 6.3766\n",
      "Loss: 2.2233\n",
      "Loss: 3.7946\n",
      "Loss: 4.3888\n",
      "Loss: 7.1028\n",
      "Loss: 4.1579\n",
      "Loss: 3.6367\n",
      "Loss: 5.5131\n",
      "Loss: 4.9484\n",
      "Loss: 3.1037\n",
      "Loss: 3.5079\n",
      "Loss: 6.4656\n",
      "Loss: 2.8636\n",
      "Loss: 4.0318\n",
      "Loss: 3.6778\n",
      "Loss: 5.0035\n",
      "Loss: 4.4113\n",
      "Loss: 5.8327\n",
      "Loss: 3.8199\n",
      "Loss: 2.1000\n",
      "Loss: 3.7382\n",
      "Loss: 8.7274\n",
      "Loss: 6.4333\n",
      "Loss: 4.7818\n",
      "Loss: 5.5427\n",
      "Loss: 5.1929\n",
      "Loss: 7.9669\n",
      "Loss: 2.9894\n",
      "Loss: 7.9695\n",
      "Loss: 4.7487\n",
      "Loss: 5.3382\n",
      "Loss: 4.2240\n",
      "Loss: 3.7708\n",
      "Loss: 5.3853\n",
      "Loss: 4.4199\n",
      "Loss: 3.8953\n",
      "Loss: 5.7933\n",
      "Loss: 3.3318\n",
      "Loss: 2.2001\n",
      "Loss: 3.5747\n",
      "Loss: 3.7170\n",
      "Loss: 5.6381\n",
      "Loss: 8.5876\n",
      "Loss: 4.3335\n",
      "Loss: 4.0458\n",
      "Loss: 4.0840\n",
      "Loss: 4.2560\n",
      "Loss: 3.4960\n",
      "Loss: 3.7771\n",
      "Loss: 4.7664\n",
      "Loss: 4.1421\n",
      "Loss: 4.4822\n",
      "Loss: 4.9244\n",
      "Loss: 5.8131\n",
      "Loss: 3.4238\n",
      "Loss: 3.7473\n",
      "Loss: 4.1409\n",
      "Loss: 4.4133\n",
      "Loss: 9.0391\n",
      "Loss: 4.8448\n",
      "Loss: 4.1497\n",
      "Loss: 7.4481\n",
      "Loss: 4.5496\n",
      "Loss: 3.5434\n",
      "Loss: 3.1604\n",
      "Loss: 6.8721\n",
      "Loss: 3.9898\n",
      "Loss: 4.6654\n",
      "Loss: 3.6309\n",
      "Loss: 4.0100\n",
      "Loss: 5.5244\n",
      "Loss: 4.5688\n",
      "Loss: 4.7499\n",
      "Loss: 3.3428\n",
      "Loss: 5.0869\n",
      "Loss: 7.0682\n",
      "Loss: 4.7367\n",
      "Loss: 6.4855\n",
      "Loss: 7.4740\n",
      "Loss: 5.4814\n",
      "Loss: 3.4145\n",
      "Loss: 7.5891\n",
      "Loss: 4.3241\n",
      "Loss: 3.5504\n",
      "Loss: 4.0687\n",
      "Loss: 3.1913\n",
      "Loss: 10.5590\n",
      "Loss: 6.2690\n",
      "Loss: 5.8409\n",
      "Loss: 4.6880\n",
      "Loss: 4.6397\n",
      "Loss: 4.0840\n",
      "Loss: 5.0790\n",
      "Loss: 4.7807\n",
      "Loss: 4.1206\n",
      "Loss: 4.7752\n",
      "Loss: 8.0302\n",
      "Loss: 7.8470\n",
      "Loss: 4.0726\n",
      "Loss: 3.9314\n",
      "Loss: 2.8526\n",
      "Loss: 4.3529\n",
      "Loss: 5.2360\n",
      "Loss: 3.8882\n",
      "Loss: 10.0701\n",
      "Loss: 7.2854\n",
      "Loss: 6.8381\n",
      "Loss: 4.0118\n",
      "Loss: 6.8631\n",
      "Loss: 6.2018\n",
      "Loss: 4.0096\n",
      "Loss: 2.9847\n",
      "Loss: 3.8602\n",
      "Loss: 6.3011\n",
      "Loss: 5.8268\n",
      "Loss: 4.3673\n",
      "Loss: 5.4547\n",
      "Loss: 4.0606\n",
      "Loss: 3.7925\n",
      "Loss: 4.0042\n",
      "Loss: 4.0756\n",
      "Loss: 4.4948\n",
      "Loss: 7.4921\n",
      "Loss: 3.5554\n",
      "Loss: 5.4269\n",
      "Loss: 5.2236\n",
      "Loss: 5.3160\n",
      "Loss: 4.4470\n",
      "Loss: 8.3880\n",
      "Loss: 4.4449\n",
      "Loss: 2.3033\n",
      "Loss: 5.2062\n",
      "Loss: 6.8486\n",
      "Loss: 3.4541\n",
      "Loss: 3.0414\n",
      "Loss: 6.4412\n",
      "Loss: 3.6401\n",
      "Loss: 4.6823\n",
      "Loss: 7.6268\n",
      "Loss: 3.5393\n",
      "Loss: 4.9833\n",
      "Loss: 3.5871\n",
      "Loss: 3.3230\n",
      "Loss: 4.1123\n",
      "Loss: 6.1786\n",
      "Loss: 4.0740\n",
      "Loss: 4.8405\n",
      "Loss: 4.3088\n",
      "Loss: 6.0666\n",
      "Loss: 5.7663\n",
      "Loss: 4.1663\n",
      "Loss: 6.2621\n",
      "Loss: 6.2442\n",
      "Loss: 4.6532\n",
      "Loss: 5.2295\n",
      "Loss: 5.0080\n",
      "Loss: 3.3108\n",
      "Loss: 4.0484\n",
      "Loss: 4.6801\n",
      "Loss: 4.7074\n",
      "Loss: 7.9175\n",
      "Loss: 4.0754\n",
      "Loss: 4.7474\n",
      "Loss: 3.7457\n",
      "Loss: 3.4677\n",
      "Loss: 4.8008\n",
      "Loss: 4.2653\n",
      "Loss: 3.6426\n",
      "Loss: 5.5854\n",
      "Loss: 4.2566\n",
      "Loss: 5.2191\n",
      "Loss: 5.1739\n",
      "Loss: 4.5498\n",
      "Loss: 4.9683\n",
      "Loss: 9.7069\n",
      "Loss: 4.3100\n",
      "Loss: 7.8794\n",
      "Loss: 4.6925\n",
      "Loss: 6.2709\n",
      "Loss: 6.0403\n",
      "Loss: 5.8491\n",
      "Loss: 5.0677\n",
      "Loss: 4.5602\n",
      "Loss: 4.8315\n",
      "Loss: 6.3541\n",
      "Loss: 4.7679\n",
      "Loss: 4.5583\n",
      "Loss: 6.2985\n",
      "Loss: 3.4581\n",
      "Loss: 7.3329\n",
      "Loss: 6.9477\n",
      "Loss: 5.9806\n",
      "Loss: 4.5566\n",
      "Loss: 4.8557\n",
      "Loss: 4.6783\n",
      "Loss: 4.3338\n",
      "Loss: 7.0002\n",
      "Loss: 5.3269\n",
      "Loss: 3.9820\n",
      "Loss: 5.2406\n",
      "Loss: 4.7140\n",
      "Loss: 5.8816\n",
      "Loss: 4.9488\n",
      "Loss: 4.3311\n",
      "Loss: 5.0165\n",
      "Loss: 3.9650\n",
      "Loss: 2.7868\n",
      "Loss: 3.4974\n",
      "Loss: 4.9233\n",
      "Loss: 5.8209\n",
      "Loss: 5.1692\n",
      "Loss: 5.8121\n",
      "Loss: 3.2323\n",
      "Loss: 5.9043\n",
      "Loss: 3.3852\n",
      "Loss: 3.5224\n",
      "Loss: 5.1474\n",
      "Loss: 5.7017\n",
      "Loss: 6.9421\n",
      "Loss: 4.0580\n",
      "Loss: 7.4004\n",
      "Loss: 2.2901\n",
      "Loss: 4.5991\n",
      "Loss: 5.7549\n",
      "Loss: 5.3223\n",
      "Loss: 4.6717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 108\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset)\u001b[0m\n\u001b[0;32m    105\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    106\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 108\u001b[0m agg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m boards\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m total    \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m boards\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Key `mlp_head` is invalid, expected torch.Tensor but received <class 'torch.nn.modules.container.Sequential'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(piece_tensors):\n\u001b[0;32m     19\u001b[0m     state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpiece_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[1;32m---> 21\u001b[0m \u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchess_model_3k.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\safetensors\\torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\safetensors\\torch.py:470\u001b[0m, in \u001b[0;36m_flatten\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is invalid, expected torch.Tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    473\u001b[0m         invalid_tensors\u001b[38;5;241m.\u001b[39mappend(k)\n",
      "\u001b[1;31mValueError\u001b[0m: Key `mlp_head` is invalid, expected torch.Tensor but received <class 'torch.nn.modules.container.Sequential'>"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import save_file, save_model\n",
    "\n",
    "# Remove these lines (they cause shared memory issues)\n",
    "# \"E\": E, \"WR\": WR, ..., \"BB2\": BB2\n",
    "\n",
    "# Use only the indexed loop version:\n",
    "state_dict = {\n",
    "    \"turn_weights\": turn_weights,\n",
    "    \"queryW\": queryW,\n",
    "    \"keyW\": keyW,\n",
    "    \"valueW\": valueW,\n",
    "    \"rope_sin\": rope_sin,\n",
    "    \"rope_cos\": rope_cos,\n",
    "    \"mlp_head\": mlp_head\n",
    "    \n",
    "}\n",
    "\n",
    "for idx, tensor in enumerate(piece_tensors):\n",
    "    state_dict[f\"piece_{idx}\"] = tensor\n",
    "\n",
    "save_file(state_dict, \"chess_model_3k.safetensors\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
